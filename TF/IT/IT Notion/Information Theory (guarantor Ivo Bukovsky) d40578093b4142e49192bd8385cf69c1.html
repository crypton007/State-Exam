<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Information Theory (guarantor Ivo Bukovsky)</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
    margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-default_background {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 237, 214, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 237, 214, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-transparentGray { background-color: rgba(227, 226, 224, 0); }
.select-value-color-translucentGray { background-color: rgba(0, 0, 0, 0.06); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(249, 228, 188, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="d4057809-3b41-42e4-9192-bd8385cf69c1" class="page sans"><header><img class="page-cover-image" src="https://www.notion.so/images/page-cover/rijksmuseum_jan_lievens_1627.jpg" style="object-position:center 60%"/><div class="page-header-icon page-header-icon-with-cover"><span class="icon">ℹ️</span></div><h1 class="page-title"><mark class="highlight-teal_background"><strong>Information Theory (guarantor Ivo Bukovsky)</strong></mark></h1><p class="page-description"></p></header><div class="page-body"><p id="ffff532a-f6dc-813a-943c-fb5e3070daea" class=""><strong>Topics: </strong></p><ul id="ffff532a-f6dc-8143-b43c-debd7320a0c9" class="bulleted-list"><li style="list-style-type:disc">Probability in information theory (conditional probability, joint probability), accuracy and robustness of machine learning classifiers, confusion matrix, ROC, Information, Entropy.</li></ul><ul id="ffff532a-f6dc-8108-9004-de3b9a6a06c1" class="bulleted-list"><li style="list-style-type:disc">Dependency and uncertainty in training data(linear correlation, mutual information, false neighborhood), anomaly detection, machine learning-based anomaly detection.</li></ul><ul id="ffff532a-f6dc-81f4-9d33-d129dee4d768" class="bulleted-list"><li style="list-style-type:disc">Clustering, low-dimensional representation of high-dimensional data, data compression, (Principal Component Analysis (PCA), SVD, t-SNE, Self Organizing Maps)</li></ul><ul id="ffff532a-f6dc-8149-ad13-ef514d0499a3" class="bulleted-list"><li style="list-style-type:disc">Communication over a noisy channel (basic types of noisy channels, information conveyed by a channel, the noisy-channel coding theorem)</li></ul><ul id="ffff532a-f6dc-815e-a3a7-f21b53f1e73a" class="bulleted-list"><li style="list-style-type:disc">Coding theory, Huffman coding, concepts of arithmetic coding, Hamming codes</li></ul><ul id="a38cd4be-1007-4352-9f3d-ff9b23153cca" class="toggle"><li><details open=""><summary><strong>Payal/Bilal Notes</strong></summary><figure id="ffff532a-f6dc-81f0-9960-d2604194b2af"><div class="source"><a href="Information_Theory_f5045eeec67f4d8286f3b282c59b67d9.pdf">Information Theory f5045eeec67f4d8286f3b282c59b67d9.pdf</a></div></figure></details></li></ul><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0"> Probability in information theory (conditional probability, joint probability), accuracy and robustness of machine learning classifiers, confusion matrix, ROC, Information, Entropy.</summary><div class="indented"><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="125c9fbf-71df-4b27-8112-10f822409ac1"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="ffff532a-f6dc-816b-b5c8-f268f9adc34c" class="">Information theory is a mathematical theory to <strong>model and analyze how information is transferred.</strong></p></div></figure><ul id="ffff532a-f6dc-8175-ae79-d0075275610a" class="toggle"><li><details open=""><summary><strong>Probability Concepts</strong>:</summary><ul id="ffff532a-f6dc-810d-aa9d-ffd5c3a196a8" class="toggle"><li><details open=""><summary><strong>Conditional Probability</strong></summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="932ae0c1-9519-4ed9-9c67-0596c8e6312d"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="775cb559-80ce-4377-99f2-4cc7f439ead4" class=""><strong>Definition</strong>: Conditional probability is the probability of one event occurring given that another event has already occurred.</p></div></figure><figure id="44597290-3312-4ee7-9579-107094a84df8" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>⁍</mtext></mrow><annotation encoding="application/x-tex">⁍</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;"></span><span class="mord">⁍</span></span></span></span></span></div></figure><ul id="ffff532a-f6dc-81dd-b817-c0129eff0d52" class="bulleted-list"><li style="list-style-type:disc">P(A∣B): Probability of event A given B has occurred.</li></ul><ul id="43a09fd7-787a-42fa-9306-983d43a04060" class="bulleted-list"><li style="list-style-type:disc">P(A∩B): Joint probability of both events occurring.</li></ul><ul id="765780c9-91de-4da8-bac4-536f2159d9bc" class="bulleted-list"><li style="list-style-type:disc">P(B): Probability of B occurring.</li></ul><p id="ffff532a-f6dc-81da-ac65-dd3ab1719a8f" class=""><strong>Example</strong>: </p><p id="67f42069-0658-4ab5-a430-bf6cc46e809b" class="">The probability of drawing an Ace from a deck of cards given that a red card has already been drawn.</p><p id="72c48a5b-5306-4850-af73-495e7524353d" class=""><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>Ace | Red Card</mtext><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>Ace</mtext><mo>∩</mo><mtext>Red Card</mtext><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>Red Card</mtext><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">P(\text{Ace | Red Card}) = \frac{P(\text{Ace} \cap \text{Red Card})}{P(\text{Red Card})}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">Ace | Red Card</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.53em;vertical-align:-0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mopen mtight">(</span><span class="mord text mtight"><span class="mord mtight">Red Card</span></span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mopen mtight">(</span><span class="mord text mtight"><span class="mord mtight">Ace</span></span><span class="mbin mtight">∩</span><span class="mord text mtight"><span class="mord mtight">Red Card</span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span><span>﻿</span></span></p></details></li></ul><ul id="550731f2-5a71-4abc-aad3-2e4706ec7329" class="toggle"><li><details open=""><summary><strong>Bayes’ Theorem</strong></summary><figure id="ffff532a-f6dc-81b9-b8ce-dc1d55176507" class="image"><a href="bayes-theorem.png"><img style="width:480px" src="bayes-theorem.png"/></a></figure><p id="ffff532a-f6dc-81ea-b0c9-c584ffce9666" class="">Bayes’ Theorem is used to update the probability of a hypothesis based on new evidence. The formula is:</p><figure id="ffff532a-f6dc-8143-a97d-f7ae852bcc1a" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>A</mi><mi mathvariant="normal">∣</mi><mi>B</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>B</mi><mi mathvariant="normal">∣</mi><mi>A</mi><mo stretchy="false">)</mo><mo>⋅</mo><mi>P</mi><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">A</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mord">∣</span><span class="mord mathnormal">A</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">A</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div></figure><ul id="732634c3-35db-4a7a-ba30-57a1d9921870" class="bulleted-list"><li style="list-style-type:disc"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>A</mi><mi mathvariant="normal">∣</mi><mi>B</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(A|B)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">A</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mclose">)</span></span></span></span></span><span>﻿</span></span> : Posterior probability, the probability of A given B (what we want to find).</li></ul><ul id="78e55a70-9178-4872-856e-4d690c31f290" class="bulleted-list"><li style="list-style-type:disc"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>B</mi><mi mathvariant="normal">∣</mi><mi>A</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(B|A)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mord">∣</span><span class="mord mathnormal">A</span><span class="mclose">)</span></span></span></span></span><span>﻿</span></span> : Likelihood, the probability of B given A.</li></ul><ul id="cb9cf99f-42fd-492c-b6d0-9e586a23d9a3" class="bulleted-list"><li style="list-style-type:disc">P(A): Prior probability, the initial probability of A (before seeing the evidence).</li></ul><ul id="ffff532a-f6dc-811f-90bc-d590ed68bef7" class="bulleted-list"><li style="list-style-type:disc">P(B): Evidence or marginal likelihood, the total probability of observing B (can be computed using all possible causes of B).</li></ul><p id="6088435a-c22b-4ca1-b4dd-49b3c5a8a9f5" class="">Example:</p><p id="6bfdd3cb-3e8d-4afb-8c2b-96d5b81a549c" class="">In a medical context, let AAA represent the event that a patient has a disease, and BBB represent the event that the patient tests positive for the disease. If the test is dependent on whether the patient has the disease, then:</p><figure id="ffff532a-f6dc-8125-8f84-ff635c4792e6" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>Has Disease | Positive Test</mtext><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>Positive Test | Has Disease</mtext><mo stretchy="false">)</mo><mo>⋅</mo><mi>P</mi><mo stretchy="false">(</mo><mtext>Has Disease</mtext><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>Positive Test</mtext><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">P(\text{Has Disease | Positive Test}) = \frac{P(\text{Positive Test | Has Disease}) \cdot P(\text{Has Disease})}{P(\text{Positive Test})}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">Has Disease | Positive Test</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">Positive Test</span></span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">Positive Test | Has Disease</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">Has Disease</span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div></figure><p id="ffff532a-f6dc-810b-bd9b-f3a5f5f03b56" class="">Here, knowing that the patient tested positive affects our belief about whether they have the disease.</p></details></li></ul><ul id="b36f8097-d814-4c2d-afeb-087c3486f4e4" class="toggle"><li><details open=""><summary><strong>Joint Probability</strong></summary><p id="ffff532a-f6dc-81ca-a1a8-cb533d25d488" class=""> Joint probability is the probability of two events occurring together.</p><figure id="ffff532a-f6dc-8142-a3e8-c8a923bd00d0" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>A</mi><mo>∩</mo><mi>B</mi><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mi>A</mi><mo>∣</mo><mi>B</mi><mo stretchy="false">)</mo><mo>⋅</mo><mi>P</mi><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(A∩B)=P(A∣B)⋅P(B)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mclose">)</span></span></span></span></span></div></figure><p id="ffff532a-f6dc-81fb-be0d-e9c2c2583fd1" class="">where </p><ul id="7e1b88ec-53e8-41b8-80f8-ed6897c5afa2" class="bulleted-list"><li style="list-style-type:disc">P(A∩B) : Probability of both events A and B happening.</li></ul><ul id="7b14f4d7-9a41-4d8e-a65a-74207951578e" class="bulleted-list"><li style="list-style-type:disc"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>A</mi><mo>∣</mo><mi>B</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(A∣B)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mclose">)</span></span></span></span></span><span>﻿</span></span> : Conditional probability of A given B.</li></ul><p id="ffff532a-f6dc-813e-b568-f9d132abcd25" class=""><strong>Example: </strong>The probability of getting heads on a coin toss and rolling a 4 on a die.</p><figure id="8917eec3-095d-412d-886b-1451b37a6d2f" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>Heads</mtext><mo>∩</mo><mtext>Roll 4</mtext><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mtext>Heads</mtext><mo stretchy="false">)</mo><mo>⋅</mo><mi>P</mi><mo stretchy="false">(</mo><mtext>Roll 4</mtext><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>×</mo><mfrac><mn>1</mn><mn>6</mn></mfrac><mo>=</mo><mfrac><mn>1</mn><mn>12</mn></mfrac></mrow><annotation encoding="application/x-tex">P(\text{Heads} \cap \text{Roll 4}) = P(\text{Heads}) \cdot P(\text{Roll 4}) = \frac{1}{2} \times \frac{1}{6} = \frac{1}{12}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">Heads</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Roll 4</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">Heads</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">Roll 4</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.0074em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.0074em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">6</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.0074em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">12</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div></figure></details></li></ul></details></li></ul><ul id="ffff532a-f6dc-81f0-9e2b-d1ef35e9af82" class="toggle"><li><details open=""><summary><strong>Machine Learning Classifier Evaluation</strong>:</summary><ul id="ffff532a-f6dc-81a9-9f9c-d156ed24c4e7" class="toggle"><li><details open=""><summary><strong>Accuracy and Robustness of Classifiers</strong></summary><ul id="ffff532a-f6dc-816a-b948-dd08351839df" class="toggle"><li><details open=""><summary><strong>Accuracy</strong></summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="ffff532a-f6dc-81d4-97c1-fa5280da6271"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="18d53a4e-a601-4074-b660-05f74fbe9828" class="">Accuracy is a measure of how often the classifier gets predictions right.</p></div></figure><p id="ffff532a-f6dc-81d7-8787-cca178abadab" class=""><strong>Accuracy Formula</strong></p><figure id="ffff532a-f6dc-8134-bf76-c5fc9cd0dc10" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Accuracy</mtext><mo>=</mo><mfrac><mrow><mtext>True Positives</mtext><mo>+</mo><mtext>True Negatives</mtext></mrow><mtext>Total Predictions</mtext></mfrac></mrow><annotation encoding="application/x-tex">\text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Predictions}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">Accuracy</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.0463em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3603em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord">Total Predictions</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord">True Positives</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord text"><span class="mord">True Negatives</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div></figure><p id="ec9c19a8-24ae-41f4-a569-6d2b25667384" class="">If we predict 100 samples and 90 are correct : 90/100=90% accuracy</p><p id="33ee248c-b7e1-4efe-ba3a-8d78f001dac1" class=""><strong>Limitations of Accuracy</strong></p><p id="2d03c1b5-ee26-4c40-96d3-0200a602c693" class="">While accuracy is a straightforward metric, it has some limitations:</p><ul id="ffff532a-f6dc-81ef-b438-f2e3b4375724" class="bulleted-list"><li style="list-style-type:disc"><strong>Imbalanced Datasets:</strong> In cases where the classes in the dataset are imbalanced (e.g., one class has significantly more examples than the other), accuracy can be misleading. A model that always predicts the majority class will have high accuracy but might not be performing well.<p id="f6dde98f-92d5-413c-8853-bf4d69d1d806" class=""><span style="border-bottom:0.05em solid">Example: </span>Consider a medical dataset where the goal is to predict whether a patient has a rare disease. In such a dataset, the number of positive cases (i.e., patients with the disease) is likely to be significantly smaller than the number of negative cases (i.e., patients without the disease).</p><p id="ffff532a-f6dc-81a1-a6a3-d6fa1250d4dc" class="">If a model simply predicts &quot;negative&quot; for all patients, it will achieve high accuracy, as the majority of cases are negative. However, this model would be completely ineffective in detecting the rare disease.</p></li></ul><ul id="ffff532a-f6dc-815f-9462-d2645146f936" class="bulleted-list"><li style="list-style-type:disc"><strong>Sensitivity to Errors:</strong> Accuracy doesn&#x27;t differentiate between different types of errors (e.g., false positives and false negatives). In some applications, one type of error might be more costly than the other.</li></ul></details></li></ul><ul id="e60000ad-b180-4caf-9205-9c068b51f939" class="toggle"><li><details open=""><summary><strong>Robustness</strong></summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="c04f93e9-3669-4bfc-8607-c3337a11e393"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="7f237b9f-b3c2-4e04-93b6-fa22c7fa9402" class="">Robustness refers to the classifier’s ability to maintain performance despite changes or challenges (noisy data, adversarial attacks, or changes in the input distribution) in the data. It is a measure of the classifier&#x27;s reliability in real-world scenarios</p></div></figure><ul id="c296dfa4-75bf-486b-8f32-7d9c541ae626" class="bulleted-list"><li style="list-style-type:disc"><strong>Key factors:</strong><ul id="d4da885d-96f9-4122-a68f-e3c32257003b" class="bulleted-list"><li style="list-style-type:circle"><strong>Data quality:</strong> noise, outliers, missing data, <strong>[imbalanced classes]</strong></li></ul><ul id="be1263a8-240a-4347-8ac3-2f9077c7bc44" class="bulleted-list"><li style="list-style-type:circle"><strong>Model complexity:</strong> overfitting, underfitting</li></ul><ul id="b581ec51-2cae-4b23-a90f-f27e0df62b72" class="bulleted-list"><li style="list-style-type:circle"><strong>Adversarial attacks</strong></li></ul><ul id="ffff532a-f6dc-8129-859b-e58284abb54d" class="bulleted-list"><li style="list-style-type:circle"><strong>Uncertainty quantification</strong></li></ul></li></ul><ul id="ffff532a-f6dc-8106-b022-d5b171d6c59d" class="bulleted-list"><li style="list-style-type:disc"><strong>Techniques:</strong> data augmentation, ensemble methods, regularization, adversarial training, Bayesian methods, certified robustness</li></ul><ul id="ffff532a-f6dc-81ce-b731-e81f7f9a350c" class="bulleted-list"><li style="list-style-type:disc"><strong>Real-world applications:</strong> medical diagnosis, autonomous vehicles, financial forecasting, natural language processing</li></ul></details></li></ul></details></li></ul><ul id="e3694337-2cbd-40d8-b1bb-20eeae011d67" class="toggle"><li><details open=""><summary><strong>Confusion Matrix</strong></summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="0b2c516d-5a40-46db-bccb-e2030eff2652"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="ffff532a-f6dc-81d9-8c2a-f0ee41eae756" class=""><strong>Helps evaluate the performance of classification models </strong></p><p id="202c0a2b-68a2-4945-a4d4-5d52cecbefdb" class=""><em><strong><span style="border-bottom:0.05em solid">It helps visualize the performance of a model by comparing the predicted labels with the actual labels</span></strong></em></p><p id="1fc050dd-8a60-45c7-95c5-1a2ca335df7a" class=""><strong>The confusion matrix helps break down the classifier’s predictions into four categories:</strong> True Positives, True Negatives, False Positives, and False Negatives.</p></div></figure><p id="59283468-4329-43cc-b646-c2d454f318d1" class=""> It provides a<strong> tabular representation of the predicted and actual classes</strong>, allowing for a detailed analysis of the <strong>model&#x27;s accuracy, precision, recall, and F1-score.</strong></p><div id="3d53fad1-9d10-47b2-8ca5-af606794204e" class="column-list"><div id="ffff532a-f6dc-816b-8cce-d2f6af3614c7" style="width:87.5%" class="column"><figure id="b5187ced-4e59-4a3b-8a87-0f7eb595f0de" class="image" style="text-align:center"><a href="confusionMatrxiUpdated.jpg"><img style="width:576px" src="confusionMatrxiUpdated.jpg"/></a></figure><p id="4d194663-fe81-46e6-b0c8-fe8fbd5e39c3" class="">
</p></div><div id="ffff532a-f6dc-81ca-aafd-d88dc4ed3af9" style="width:112.5%" class="column"><figure id="d521c4de-5bf0-49b0-a444-963b01a40f91" class="image"><a href="image.png"><img style="width:720px" src="image.png"/></a></figure></div></div><h3 id="79bb8cf5-21e1-4c95-a536-b74dd6f4921b" class="">Components of a Confusion Matrix</h3><ol type="1" id="ffff532a-f6dc-8152-a120-d93418d4f1dc" class="numbered-list" start="1"><li><strong>True Positive (TP)</strong>: The number of instances correctly predicted as positive.</li></ol><ol type="1" id="ffff532a-f6dc-81fe-9de3-da7c17a66279" class="numbered-list" start="2"><li><strong>True Negative (TN)</strong>: The number of instances correctly predicted as negative.</li></ol><ol type="1" id="ffff532a-f6dc-81b2-b406-f54e28903f60" class="numbered-list" start="3"><li><strong>False Positive (FP)</strong>: The number of instances incorrectly predicted as positive (Type I error).</li></ol><ol type="1" id="ffff532a-f6dc-8137-a01a-e03440bf0572" class="numbered-list" start="4"><li><strong>False Negative (FN)</strong>: The number of instances incorrectly predicted as negative (Type II error).</li></ol><p id="ffff532a-f6dc-81b2-8ca3-c1b8b276ba03" class=""><strong>Summary </strong>:</p><ul id="b3eeb02a-3212-459f-8616-f612f8b76740" class="bulleted-list"><li style="list-style-type:disc"><strong>Accuracy</strong>: Overall correctness.</li></ul><ul id="ffff532a-f6dc-81aa-85f8-c842fc739ac3" class="bulleted-list"><li style="list-style-type:disc"><strong>Precision(PPV, Positive predicted Value)</strong>: Accuracy of positive predictions.[<strong>High Precision</strong>: Means that when the model predicts a positive outcome, it is very likely to be correct]</li></ul><ul id="ad124f4e-d6ea-474f-afb9-f36350df486c" class="bulleted-list"><li style="list-style-type:disc"><strong>Recall or Sensitivity or True Positive Rate (TPR)</strong>: Ability to capture all positive cases.[<strong>High Recall</strong>: Means that the model is effectively capturing most of the actual positive cases]</li></ul><ul id="ffff532a-f6dc-81e4-a352-c503b24bfaed" class="bulleted-list"><li style="list-style-type:disc"><strong>F1 Score</strong>: Balance between Precision and Recall. (harmonic mean of Precision and Recall- 2*(P*R)/(P+R), useful when you need to balance the trade-off between Precision and Recall, especially when the class distribution is imbalanced.)[<strong>High F1 Score</strong>: Indicates a good balance between Precision and Recall.]</li></ul><ul id="3a4c82dd-0e7e-4e64-a7fd-9227ece638d8" class="bulleted-list"><li style="list-style-type:disc"><strong>Specificity/Ture negative Rate(TNR)</strong>: Ability to capture all negative cases. [<strong>High Specificity</strong>: Means that the model is good at identifying actual negative cases. ]</li></ul><ul id="ffff532a-f6dc-8189-a77d-cb8aedd28d63" class="bulleted-list"><li style="list-style-type:disc"><strong>Negative Predicted Value : </strong>NPV=TN/TN+FN</li></ul></details></li></ul><ul id="1ea7aeeb-589d-4009-88ad-a6659ba79e62" class="toggle"><li><details open=""><summary><strong>ROC (Receiver Operating Characteristic) Curve</strong></summary><div id="0ed564d2-ada8-4d34-984e-c71915b9bfba" class="column-list"><div id="ffff532a-f6dc-81d3-91f5-f364c767945a" style="width:100%" class="column"><figure id="339a8b3f-4c89-4ec2-a861-4c5120d5af91" class="image"><a href="image%201.png"><img style="width:480px" src="image%201.png"/></a></figure><p id="ffff532a-f6dc-8139-b865-e71eab94354d" class="">The diagonal line from (0,0) to (1,1) represents the performance of a random classifier</p><p id="ffff532a-f6dc-81e1-8bd6-ddc67c99b494" class="">A classifier that performs better than random guessing will have its ROC curve above the diagonal line.</p><p id="ef8cf29b-81a2-4884-a762-c7bbe6b4d82b" class="">The farther the curve is from the diagonal, the better the model is at distinguishing between the positive and negative classes.</p></div><div id="46d41dd7-c3d7-4802-99bf-dab147a4f6ae" style="width:100%" class="column"><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="ffff532a-f6dc-81ee-b244-dce22cfe589d"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="a9bebf51-cf58-4083-900f-1a81672ef7d3" class=""><strong>ROC Curve p</strong>lots True Positive Rate (TPR) against False Positive Rate (FPR).</p><p id="b7c64f7f-1d49-452e-9148-750801ac651c" class=""><strong>It helps in selecting the optimal threshold for a classifier.</strong></p></div></figure><figure id="b86dd46f-3a67-4515-9c69-671a1a9778c1" class="image"><a href="image%202.png"><img style="width:624px" src="image%202.png"/></a></figure><p id="de0d48ef-e1c8-4b41-ae46-0dc58835faab" class=""><strong>TPR</strong>=ratio of correctly predicted positive cases to the total actual positive cases.</p><p id="f22007b7-7f69-4a24-a668-79ee4cc597c8" class=""><strong>FPR</strong>=ratio of incorrectly predicted positive cases to the total actual negative cases.</p><p id="ffff532a-f6dc-8123-9d9e-f64a893e14f6" class=""><strong>How ROC is constructed:</strong></p><ul id="4bec9b47-f03a-487a-b7f9-6479031fe8d6" class="bulleted-list"><li style="list-style-type:disc"><strong>Threshold Variation</strong>: The ROC curve is plotted by <em><span style="border-bottom:0.05em solid">varying the decision threshold</span></em> of the classifier. <em><span style="border-bottom:0.05em solid">For each threshold, the model calculates TPR and FPR.</span></em></li></ul><ul id="f06c0d34-6695-4694-bf44-72ea5f0886f9" class="bulleted-list"><li style="list-style-type:disc">The x-axis  False Positive Rate (FPR) and  y-axis represents the True Positive Rate (TPR), point on the curve corresponds to a different threshold setting.</li></ul><ul id="ffff532a-f6dc-81fe-9a8b-c4b648c47505" class="bulleted-list"><li style="list-style-type:disc">As the threshold varies from 0 to 1, the TPR and FPR change, and the ROC curve is formed by connecting these points.</li></ul><p id="ffff532a-f6dc-8143-be9c-df976ce25b0e" class="">
</p><p id="ffff532a-f6dc-81aa-857d-fa6574f4badb" class=""><br/><br/><br/></p></div></div><p id="ffff532a-f6dc-812f-8f92-debdedbfe88b" class="">
</p></details></li></ul><ul id="ffff532a-f6dc-81bd-9174-fb43a9f0fa06" class="toggle"><li><details open=""><summary><strong>AUC Curve (Area under curve )</strong></summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="5a34c63e-727d-4e1f-b9df-78e99c458ad9"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><ul id="028ad3be-acba-40e8-a4bf-244253c31f26" class="bulleted-list"><li style="list-style-type:disc">It quantifies the overall performance of the classifier. </li></ul><ul id="d9ca90e4-f31b-4087-bb71-e3eff2bca936" class="bulleted-list"><li style="list-style-type:disc"><strong>The AUC is the area under the ROC curve.</strong></li></ul></div></figure><div id="ba8ae66b-8c31-409d-8cce-bde872ca7bd8" class="column-list"><div id="6686af0d-fa62-481d-b0d8-e630b3366736" style="width:131.25%" class="column"><figure id="f7970691-4c72-476b-95cc-4a14044fe2ed" class="image"><a href="image%203.png"><img style="width:768px" src="image%203.png"/></a></figure></div><div id="cd57d9de-7247-4af4-b3fb-f28b1f46150c" style="width:68.74999999999999%" class="column"><p id="ffff532a-f6dc-81d9-8a0d-eaa9c7e3c3cb" class="">
</p><ul id="ffff532a-f6dc-8169-98e0-f197ff923a60" class="bulleted-list"><li style="list-style-type:disc"><strong>AUC = 0.5</strong>: Indicates no discrimination (equivalent to random guessing).</li></ul><ul id="f79488d5-e1a5-4ba9-8e7c-5b872a57a8f6" class="bulleted-list"><li style="list-style-type:disc"><strong>0.5 &lt; AUC &lt; 1</strong>: Indicates some degree of discrimination.</li></ul><ul id="3c7f61f3-82f8-4b89-ae6b-f876deb516ef" class="bulleted-list"><li style="list-style-type:disc"><strong>AUC = 1</strong>: Indicates perfect discrimination (the model can perfectly distinguish between positive and negative classes).</li></ul><p id="ffff532a-f6dc-8149-93dc-f3185f26e0ed" class="">
</p><p id="ffff532a-f6dc-81a6-b8be-fecb5eb5d5f2" class=""><strong>High AUC Value:</strong> Indicates that the model is good at distinguishing between positive and negative cases.</p><p id="a054425a-75e5-4497-865f-7c12d3b6b081" class=""><strong>Low AUC Value:</strong> Indicates that the model performs poorly in distinguishing between the classes ( similar to random guessing)</p><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="ffff532a-f6dc-81f2-ba01-f9e81557f6e9"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="ffff532a-f6dc-81d8-a868-f784d7a1fc87" class="">In summary, the ROC curve provides a comprehensive view of a model’s performance across different threshold values, and the AUC offers a single scalar metric to evaluate and compare classifiers.</p></div></figure></div></div></details></li></ul></details></li></ul><ul id="ffff532a-f6dc-8176-9052-c5bb80631322" class="toggle"><li><details open=""><summary><strong>Information Theory</strong>:</summary><ul id="89863b95-790c-4748-9f77-6542d22d91be" class="toggle"><li><details open=""><summary><strong>Information and Entropy</strong></summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="ffff532a-f6dc-81c5-b120-e86a8b0aa636"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="15d58789-8d56-4ea8-8e0e-f983a95d90b7" class=""><strong>Information</strong></p><div id="ffff532a-f6dc-816e-a89b-de589cf80db0" class="column-list"><div id="ffff532a-f6dc-81ef-9fb5-f9c46e805089" style="width:100%" class="column"><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="ffff532a-f6dc-81ef-94c9-fbf1531059f4"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="8928c22d-553e-4457-83f6-8069ea6201a0" class="">Information quantifies the amount of uncertainty reduced when learning the outcome of an event</p><p id="ffff532a-f6dc-815f-853f-d2ace2edc033" class="">In information theory, information is the reduction of uncertainty. The rarer an event, the more information it provides.</p><p id="2c7b1215-7f81-4455-93a1-9f809fc08088" class=""><em>Information quantifies the &quot;surprise&quot; or &quot;uncertainty&quot; associated with the outcome of a random variable</em></p><p id="9c20da3d-7e1f-4f25-a86e-ea048b5674bb" class="">The choice of the base-2 logarithm means that the units of the information measure is in bits (binary digits)</p><p id="2b98aa43-92b1-4c0d-8200-e5da8e95da6d" class="">Information will be zero when the probability of an event is 1.0 or a certainty, e.g. there is no surprise.</p><p id="135c041d-c3c1-43d9-a7ff-115ed0484dd9" class="">If the same coin was flipped n times, then the information for this sequence of flips would be n bits</p></div></figure><p id="50f64e4f-cbfa-4531-bf09-cace818c095f" class="">
</p><p id="ffff532a-f6dc-81d3-80d8-ff1f0147207c" class=""><strong>Formula</strong>:</p><figure id="ffff532a-f6dc-81aa-832b-e7fb9c85e146" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>I</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>2</mn></msub><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">I(x) = -\log_2 P(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span></div></figure><ul id="ffff532a-f6dc-815a-b5d7-dfa71e494990" class="bulleted-list"><li style="list-style-type:disc"><strong>Where</strong>:<ul id="a039d67a-6351-4558-b915-2ae448263f42" class="bulleted-list"><li style="list-style-type:circle"><em>I(x)</em> is the amount of information gained from observing event x.</li></ul><ul id="da709e13-3f41-42b2-a1cf-3bf48fc2b748" class="bulleted-list"><li style="list-style-type:circle"><em>P(x) </em>is the probability of discrete event x.</li></ul><p id="769b2f85-60b0-40b8-b3bc-006f083dd925" class="">Log is used <strong> because it linearizes the relationship between probability and information.</strong></p><p id="965db443-8469-4a46-9776-93d8def7c154" class=""> without log, Linear change in probability leads to an exponential change in information, and the logarithm helps to linearize this relationship.</p></li></ul></div><div id="ffff532a-f6dc-8144-a1de-fb47986b57ac" style="width:100%" class="column"><figure id="ffff532a-f6dc-8104-80d0-d59b2443b26b" class="image"><a href="Plot-of-Probability-vs-Information.png"><img style="width:624px" src="Plot-of-Probability-vs-Information.png"/></a></figure><p id="d051e716-000f-4aa0-b9e9-6dd8272c4602" class="">
</p></div></div></div></figure><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="78890b96-8145-4bcd-8202-0f75390d46f6"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="ffff532a-f6dc-81fc-8d60-e1315af89104" class=""><strong>Entropy</strong></p><div id="b301226a-9598-4346-9163-db7562db6aa1" class="column-list"><div id="3e0b37c9-5047-4658-bf2a-1e3fee3756ca" style="width:100%" class="column"><p id="ffff532a-f6dc-81fc-8a70-c2aadf74253d" class=""><strong>Entropy is a measure of the uncertainty associated with a random variable. A higher entropy indicates greater uncertainty.</strong></p><p id="934a7156-b7ad-491b-aae9-152e15b2d9e2" class="">Average information content : Entropy represents the average amount of uncertainty or surprise associated with a set of possible outcomes<br/>It is the expected value of surprise<br/></p><p id="ffff532a-f6dc-819c-b1e0-c4ea36411626" class="">Youtube Video (Watch again) :<a href="https://www.youtube.com/watch?app=desktop&amp;v=0GCGaw0QOhA">https://www.youtube.com/watch?app=desktop&amp;v=0GCGaw0QOhA</a> </p><figure id="ffff532a-f6dc-819b-850f-ee5235279e8a"><div class="source"><a href="Note_5._Sep_2024_(2).pdf">https://prod-files-secure.s3.us-west-2.amazonaws.com/cc9fb959-9635-4863-80b9-5a928f55ffb0/f9fe9982-f74b-400b-a54d-0f1bceb5b4ba/Note_5._Sep_2024_(2).pdf</a></div></figure></div><div id="ffff532a-f6dc-812c-94c2-e3df080f923f" style="width:100%" class="column"><figure id="b2c43aeb-e18a-4a63-a209-30ab7af16d80" class="image"><a href="image%204.png"><img style="width:720px" src="image%204.png"/></a></figure><p id="ffff532a-f6dc-817b-bc7d-fabc218fdca6" class="">
</p><figure id="ffff532a-f6dc-81c8-877a-ffd8dced76cd" class="image"><a href="0_8DPGXL_lLGq8zGrc.webp"><img style="width:480px" src="0_8DPGXL_lLGq8zGrc.webp"/></a></figure><p id="ffff532a-f6dc-81e1-8dbe-e6fa07320b3e" class="">
</p></div></div></div></figure><p id="ffff532a-f6dc-8133-a16e-e50314b02a0a" class="">
</p></details></li></ul></details></li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0">Dependency and uncertainty in training data, linear correlation, mutual information, false neighborhood), anomaly detection, machine learning-based anomaly detection.</summary><div class="indented"><ul id="233f4405-b040-4df4-8034-4a321f06819d" class="toggle"><li><details open=""><summary><strong>Dependency and Uncertainty in Training Data</strong>:</summary><ul id="ffff532a-f6dc-816d-a065-db2cc680c522" class="toggle"><li><details open=""><summary><strong>Dependency and Uncertainty</strong></summary><div id="ffff532a-f6dc-814c-b7e2-f0dfea886114" class="column-list"><div id="2f8881e9-763b-4107-9056-fed54d38136e" style="width:106.25%" class="column"><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="ca115cba-52b9-40e7-8e9f-8957ef7e6c1e"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="d656ec00-08e8-47ff-9565-e3545fc157ce" class=""><strong><span style="border-bottom:0.05em solid">Dependency in Training Data  </span></strong></p><p id="ffff532a-f6dc-817b-9ec9-e01d37bf11fb" class="">Dependency refers to the <strong><span style="border-bottom:0.05em solid">relationship between features in the dataset</span></strong>. If one feature depends on another, it implies that knowing the value of one feature provides information about the value of the other.</p><p id="539abd08-46c1-42b5-8b0a-3efd3b8d3115" class=""><strong>Types of Dependency: </strong></p><ul id="3a28c6d1-9493-4207-a530-9ede56ab4c38" class="bulleted-list"><li style="list-style-type:disc"><strong>Linear Dependency</strong>: Features are linearly related (e.g., <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><msub><mi>β</mi><mn>0</mn></msub><mo>+</mo><msub><mi>β</mi><mn>1</mn></msub><mi>x</mi></mrow><annotation encoding="application/x-tex">y = \beta_0 + \beta_1 x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal">x</span></span></span></span></span><span>﻿</span></span>).<ul id="ffff532a-f6dc-818b-9b2e-c9da307c9ca3" class="bulleted-list"><li style="list-style-type:circle">Example: House price with room and area size relation</li></ul></li></ul><ul id="ffff532a-f6dc-81ba-afe1-c9da01912779" class="bulleted-list"><li style="list-style-type:disc"><strong>Non Linear Dependency</strong>:  Relationships that are not linear (e.g., <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><msub><mi>β</mi><mn>0</mn></msub><mo>+</mo><msub><mi>β</mi><mn>1</mn></msub><mi>x</mi><mo>+</mo><msub><mi>β</mi><mn>2</mn></msub><msup><mi>x</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">y = \beta_0 + \beta_1 x + \beta_2 x^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span>).<ul id="ce92d5c4-c555-4db3-ac33-aa8fc508af4a" class="bulleted-list"><li style="list-style-type:circle">Example: Population growth in the city (exponential pattern)</li></ul></li></ul><p id="ffff532a-f6dc-8188-a4b7-c5ca2d906658" class=""><strong>Detection:</strong></p><p id="ffff532a-f6dc-81f5-8944-dd74506a0f46" class="">Techniques include:</p><ul id="5fc79b1d-b8c5-4ee0-8463-65f1ef8e8f78" class="bulleted-list"><li style="list-style-type:disc"><strong>Correlation Analysis:</strong> Using Pearson or Spearman correlation coefficients.</li></ul><ul id="ffff532a-f6dc-8121-824d-ce18bd55e6c1" class="bulleted-list"><li style="list-style-type:disc"><strong>Graphical Methods:</strong> Scatter plots or pairwise plots.</li></ul></div></figure></div><div id="3ade6c0e-46c2-4f6d-b49c-8f9ab051a1d9" style="width:87.50000000000001%" class="column"><figure class="block-color-pink_background callout" style="white-space:pre-wrap;display:flex" id="ffff532a-f6dc-819b-b823-d3b9fe3f8916"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="e324e5f0-9804-46db-9fe1-d534369d59a8" class=""><strong><span style="border-bottom:0.05em solid">Uncertainty in Training Data  </span></strong></p><p id="cbb85e1c-3bd7-4dc5-b54b-d840bb96fd20" class="">Uncertainty refers to the <strong><span style="border-bottom:0.05em solid">lack of certainty in the training data</span></strong>, which could be due to noise, incomplete data, or inherent randomness.</p><p id="cfd6474d-f68b-45a6-94f9-0af7dda1950f" class=""><strong><span style="border-bottom:0.05em solid">Measures:</span></strong></p><ul id="ffff532a-f6dc-81ad-98e9-d1ebba143f77" class="bulleted-list"><li style="list-style-type:disc"><strong>Entropy:</strong> A measure of the average information content of a probability distribution.</li></ul><ul id="ffff532a-f6dc-8119-a5ee-ce5a08b27fb5" class="bulleted-list"><li style="list-style-type:disc"><strong>Conditional entropy:</strong> The entropy of a variable given the value of another variable.</li></ul><ul id="2eecee50-9a2d-476b-a800-574438177845" class="bulleted-list"><li style="list-style-type:disc"><strong>Mutual information:</strong> A measure of the mutual dependence between two variables.</li></ul></div></figure><p id="9dccf177-fa21-4f1c-86e5-7f418bc283c3" class="">
</p><p id="2a6962c6-f133-4b4c-b872-2e4079fc072a" class="">
</p></div></div></details></li></ul><ul id="ffff532a-f6dc-8170-bcfb-f426125ba6b9" class="toggle"><li><details open=""><summary><strong>Linear Correlation</strong></summary><p id="ffff532a-f6dc-8154-9e2b-e4ed25400991" class=""><em><strong><span style="border-bottom:0.05em solid">Describes the relationship between two variables in a linear manner</span></strong></em>, meaning that changes in one variable result in proportional changes in the other variable.</p><p id="bbd7e660-10c3-4a90-ac6a-608a287c6a32" class=""><strong><span style="border-bottom:0.05em solid">Purpose:</span></strong></p><ul id="38ee8e22-f9f6-4d85-bddc-3d1ac750dc3b" class="bulleted-list"><li style="list-style-type:disc"><strong>Strength of Relationship:</strong> Indicates how strongly the variables are related.</li></ul><ul id="ffff532a-f6dc-81fb-84d1-f740a821656e" class="bulleted-list"><li style="list-style-type:disc"><strong>Direction of Relationship:</strong> Shows whether the relationship is positive or negative.</li></ul><div id="22bdf9f2-b54f-42ba-a556-84de6851c923" class="column-list"><div id="ffff532a-f6dc-819f-a400-cbda53aad069" style="width:106.25%" class="column"><p id="b2055aa1-ca56-4968-a17f-90a4fc793ef3" class="block-color-blue_background"><strong><span style="border-bottom:0.05em solid">Pearson Correlation Coefficient</span></strong></p><ul id="ffff532a-f6dc-816b-b3cf-d4dc1681319c" class="bulleted-list"><li style="list-style-type:disc">Measures the <strong>strength and direction of the </strong><strong><span style="border-bottom:0.05em solid">linear relationship</span></strong> between two continuous variables.</li></ul><ul id="ffff532a-f6dc-81a6-a8b9-cc65620afca9" class="bulleted-list"><li style="list-style-type:disc"><strong>Assumptions:</strong><ul id="ffff532a-f6dc-8137-8cd5-caa706de3ebc" class="bulleted-list"><li style="list-style-type:circle"><strong>Linearity:</strong> Assumes a linear relationship between the variables.</li></ul><ul id="1477ec1a-5693-4900-9284-c4fc51c21a55" class="bulleted-list"><li style="list-style-type:circle"><strong>Normality:</strong> Assumes that the variables are approximately normally distributed.</li></ul></li></ul><ul id="ffff532a-f6dc-81ee-bfd8-f31486edc6c1" class="bulleted-list"><li style="list-style-type:disc"><strong>Range:</strong><mark class="highlight-yellow_background"><span style="border-bottom:0.05em solid"> r ranges from -1 to 1 </span></mark><p id="9457ed5f-24cb-46d8-b6e1-0dbf47ecba2f" class="">where <div class="indented"><p id="ffff532a-f6dc-81e9-a355-c68e4f0e1daa" class=""><mark class="highlight-pink"><strong>r=</strong></mark><mark class="highlight-pink"><strong><span style="border-bottom:0.05em solid">1: </span></strong></mark><mark class="highlight-pink"><span style="border-bottom:0.05em solid">Perfect positive</span></mark><mark class="highlight-pink"> linear relationship.</mark></p><p id="17debd0a-241f-4e98-9a70-f1c8f68e3040" class=""><mark class="highlight-pink"><strong>r=</strong></mark><mark class="highlight-pink"><strong><span style="border-bottom:0.05em solid">−1:</span></strong></mark><mark class="highlight-pink"><span style="border-bottom:0.05em solid">  Perfect negative</span></mark><mark class="highlight-pink"> linear relationship.</mark></p><p id="ffff532a-f6dc-814a-8ad8-e442641476f0" class=""><mark class="highlight-pink"><strong>r=</strong></mark><mark class="highlight-pink"><strong><span style="border-bottom:0.05em solid">0: </span></strong></mark><mark class="highlight-pink"><span style="border-bottom:0.05em solid">No linear relationship.</span></mark></p></div></p></li></ul><ul id="ffff532a-f6dc-8135-919c-efb85055004a" class="bulleted-list"><li style="list-style-type:disc"><strong>Formula </strong>: <figure id="5aebb962-a59c-4806-9342-0c728117d8f8" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>r</mi><mo>=</mo><mfrac><mrow><mtext>Cov</mtext><mo stretchy="false">(</mo><mi>X</mi><mo separator="true">,</mo><mi>Y</mi><mo stretchy="false">)</mo></mrow><mrow><msub><mi>σ</mi><mi>X</mi></msub><msub><mi>σ</mi><mi>Y</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">r = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.263em;vertical-align:-0.836em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em;">X</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">Y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord">Cov</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.836em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div></figure><ul id="ffff532a-f6dc-81b9-9bd1-ed71c2a21f50" class="bulleted-list"><li style="list-style-type:circle">Where Cov(X,Y) is the covariance of X and Y.</li></ul><ul id="ffff532a-f6dc-81de-9c52-d6942aaa80e5" class="bulleted-list"><li style="list-style-type:circle">σXand σY​ are the standard deviations of X and Y.</li></ul><figure id="3cb3864b-e62c-48a3-b48d-9ed42af6bbed" class="image"><a href="images.png"><img style="width:432px" src="images.png"/></a></figure></li></ul><ul id="c7b0540d-d375-4d46-b444-f1ce3a9c9127" class="bulleted-list"><li style="list-style-type:disc"><strong>Sensitive to Outliers</strong></li></ul><ul id="ffff532a-f6dc-8109-a2df-ce482353a98a" class="bulleted-list"><li style="list-style-type:disc"><strong>Symmetry: </strong>The correlation between X and Y is the same as between Y and X (i.e., r(X, Y) = r(Y, X)</li></ul></div><div id="ffff532a-f6dc-81ab-838b-dd92bc377612" style="width:93.74999999999999%" class="column"><p id="350b0aa2-14b0-4063-bb32-99271e4d6101" class="block-color-teal_background"><strong><span style="border-bottom:0.05em solid">Spearman’s Rank Coefficient </span></strong><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ρ</mi></mrow><annotation encoding="application/x-tex">\rho</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">ρ</span></span></span></span></span><span>﻿</span></span></p><ul id="8de03731-4cc4-4efc-aff4-e52cdb234c61" class="bulleted-list"><li style="list-style-type:disc">Measures the strength and direction of a <strong>monotonic relationship</strong> between two variables using their ranks.<blockquote id="db2de690-a224-436c-8773-8e0fb52ea1d6" class=""><span style="border-bottom:0.05em solid">Monotonic </span>: If one variable increases, the other variable tends to increase (or decrease) consistently, <em><span style="border-bottom:0.05em solid">but not necessarily in a linear fashion.</span></em></blockquote></li></ul><ul id="7e77de54-ba86-476c-a934-5bfb49deefb8" class="bulleted-list"><li style="list-style-type:disc"><strong>Assumptions</strong><ul id="ffff532a-f6dc-81f1-a91a-d5058a314102" class="bulleted-list"><li style="list-style-type:circle"><strong>Monotonic Relationship:</strong> Assumes a monotonic relationship, where variables consistently increase or decrease together, but not necessarily in a linear manner.</li></ul><ul id="ffff532a-f6dc-81d6-ae92-e30eec276da3" class="bulleted-list"><li style="list-style-type:circle"><strong>Ordinal Data:</strong> Can handle ordinal data and does not require the data to be normally distributed.</li></ul></li></ul><ul id="65b3ef97-2dcf-4177-aa53-1277ebf59de1" class="bulleted-list"><li style="list-style-type:disc"><strong> Formula:</strong><figure id="129e6535-eaf9-4ec6-a9e1-eaa889dfa3b4" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>ρ</mi><mo>=</mo><mn>1</mn><mo>−</mo><mfrac><mrow><mn>6</mn><mo>∑</mo><msubsup><mi>d</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mrow><mi>n</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\rho = 1 - \frac{6 \sum d_i^2}{n(n^2 - 1)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">ρ</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.4271em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.4911em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7401em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">6</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div></figure><p id="ffff532a-f6dc-81c6-bfc0-c8d4577f5c9a" class=""> Where di is the difference between the ranks of the paired variables, and n is the number of pairs. </p><p id="ffff532a-f6dc-81ac-8468-f4d229ac05b7" class=""><strong>Example</strong>: <a href="https://www.analyticsvidhya.com/blog/2021/03/comparison-of-pearson-and-spearman-correlation-coefficients/">https://www.analyticsvidhya.com/blog/2021/03/comparison-of-pearson-and-spearman-correlation-coefficients/</a></p></li></ul><ul id="c56875d2-e3e0-4b4f-b317-5ce5d0c0d162" class="bulleted-list"><li style="list-style-type:disc"><mark class="highlight-purple_background"><strong>Range: </strong></mark><mark class="highlight-purple_background">ρ ranges from -1 to 1.</mark><ul id="ffff532a-f6dc-8114-8392-fdcd5c810066" class="bulleted-list"><li style="list-style-type:circle"><mark class="highlight-purple_background"><strong>ρ=1: Perfect positive monotonic </strong></mark><mark class="highlight-purple_background">relationship.</mark></li></ul><ul id="ffff532a-f6dc-81b8-84f6-f569d1434ff7" class="bulleted-list"><li style="list-style-type:circle"><mark class="highlight-purple_background"><strong>ρ=−1: Perfect negative monotonic</strong></mark><mark class="highlight-purple_background"> relationship.</mark></li></ul><ul id="136121c8-3bc8-4fb9-a8ef-c4e0911f791c" class="bulleted-list"><li style="list-style-type:circle"><mark class="highlight-purple_background"><strong>ρ=0: No monotonic </strong></mark><mark class="highlight-purple_background">relationship.</mark></li></ul></li></ul><ul id="aed6bae4-84f2-4d7d-9bdd-b051ff340dbe" class="bulleted-list"><li style="list-style-type:disc"><strong>Robustness:</strong> More robust to outliers as it uses ranks rather than raw data.<br/><br/><br/></li></ul></div></div></details></li></ul><ul id="c2401881-3250-4ffb-97ad-49b791aeb835" class="toggle"><li><details open=""><summary><strong>Mutual Information</strong></summary><figure class="block-color-pink_background callout" style="white-space:pre-wrap;display:flex" id="ffff532a-f6dc-8195-b01c-f26e3d82a170"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="57909872-aef2-4548-aae4-3db36893414a" class=""><strong>Mutual information</strong> is a statistical measure that quantifies the dependency between two random variables. It is a measure of how much information one random variable provides about another.</p><div id="08d9d7df-41b3-486b-b790-240bfddbe1fd" class="column-list"><div id="dacc8bad-ca8f-4875-93ac-3ae1096c729e" style="width:100%" class="column"><p id="ffff532a-f6dc-8115-825d-fdfccfa12cd5" class=""><strong>Formula :</strong></p><figure id="64efe1bf-6d14-49b4-97e7-3e894c031034" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>I</mi><mo stretchy="false">(</mo><mi>X</mi><mo separator="true">;</mo><mi>Y</mi><mo stretchy="false">)</mo><mo>=</mo><mi>I</mi><mo stretchy="false">(</mo><mi>Y</mi><mo separator="true">;</mo><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>H</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>−</mo><mi>H</mi><mo stretchy="false">(</mo><mi>X</mi><mi mathvariant="normal">∣</mi><mi>Y</mi><mo stretchy="false">)</mo><mo>=</mo><mi>H</mi><mo stretchy="false">(</mo><mi>Y</mi><mo stretchy="false">)</mo><mo>−</mo><mi>H</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">I(X; Y) = I(Y; X) =H(X) - H(X | Y) = H(Y) - H(Y | X)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">)</span></span></span></span></span></div></figure><figure id="ffff532a-f6dc-81e8-b0fe-e5915b3276b5" class="image"><a href="image%205.png"><img style="width:624px" src="image%205.png"/></a></figure><p id="ffff532a-f6dc-81e3-8b13-db71d21b6f74" class="">Where </p><p id="85dbf94e-a3a6-47b0-8cba-1892c768a0c9" class=""><strong>p(x,y)</strong> is the joint probability</p><p id="3f6c13d3-a2d7-424b-8d26-3c930ee2e93b" class=""><strong>p(x), p(y)</strong> are the marginal probabilities.</p></div><div id="7daabfbd-e293-4475-bfd8-3bf16cdd2481" style="width:100%" class="column"><figure id="c9c8194b-007b-49d2-bd98-97649f572b70" class="image"><a href="1_f4gd1WKovdfSSTF3SPoDlA.png"><img style="width:254.43359375px" src="1_f4gd1WKovdfSSTF3SPoDlA.png"/></a></figure></div></div></div></figure></details></li></ul><ul id="ffff532a-f6dc-810d-9a9f-e8dfbcced080" class="toggle"><li><details open=""><summary><strong>False Neighborhoods</strong></summary><figure class="block-color-pink_background callout" style="white-space:pre-wrap;display:flex" id="a0f38ef2-79d9-4bb1-9822-6e5947222f69"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="ffff532a-f6dc-81ac-af6d-c6760473f275" class=""><strong>False neighborhood </strong>refers to a situation where two variables appear to be correlated, but their relationship is actually due to their shared dependency on a third variable.</p></div></figure><p id="ffff532a-f6dc-8102-879b-cdbfdc9fe8d4" class=""><strong>Examples:</strong></p><ul id="ffff532a-f6dc-81a1-a01e-ceaec48be846" class="bulleted-list"><li style="list-style-type:disc"><strong>Regression Example:</strong> Consider predicting house prices based on features like size and location. Two houses might be close in size but in very different locations (e.g., one in a high-demand urban area and the other in a less desirable suburb). A simple model might incorrectly predict similar prices for these houses if it only considers size.</li></ul><p id="b18ed18a-8d2e-47b7-a3a9-f616def59414" class=""><strong>Impact on Models:</strong></p><ul id="d1f933d6-454e-49a7-879d-305e56d8d015" class="bulleted-list"><li style="list-style-type:disc"><strong>Overfitting:</strong> Models may overfit to local patterns in the training data, failing to generalize well to unseen data. This happens when a model learns to recognize specific but non-generalizable patterns.</li></ul><ul id="803313dd-850d-4543-ac0c-5a0f4853a2cd" class="bulleted-list"><li style="list-style-type:disc"><strong>Poor Performance:</strong> The model&#x27;s performance might degrade when tested on new or varied data, as the false neighborhoods lead to incorrect assumptions about the relationships in the data.</li></ul><p id="5f51801a-3094-4ccb-8dbf-51639ded214a" class=""><strong>Solution:</strong></p><ul id="ffff532a-f6dc-8184-8b69-dd9646cd48a9" class="bulleted-list"><li style="list-style-type:disc"><strong>Feature Engineering:</strong> Pick relevant features.</li></ul><ul id="c30639ff-f25f-4bdb-b91c-d98c6ffb4668" class="bulleted-list"><li style="list-style-type:disc"><strong>Model Complexity:</strong> Use advanced models (e.g., polynomial regression, neural networks).</li></ul><ul id="dcb125d5-00e8-4f8e-8846-98ba90347cc2" class="bulleted-list"><li style="list-style-type:disc"><strong>Cross-Validation:</strong> Test on different data sets (k fold cross validation</li></ul><ul id="03462469-0389-48a1-8696-2b062231bd23" class="bulleted-list"><li style="list-style-type:disc"><strong>Contextual Understanding:</strong> Use domain knowledge </li></ul><p id="ffff532a-f6dc-815f-82c8-f147a206c763" class=""><strong>Advanced Techniques:</strong></p><ul id="0dedd2d9-eba3-4a94-b30a-485aac747680" class="bulleted-list"><li style="list-style-type:disc"><strong>Distance Metrics:</strong> Use advanced techniques or dimensionality reduction (like PCA) to better capture relevant patterns and avoid misleading similarities.</li></ul><ul id="ffff532a-f6dc-81a0-a001-efe8a9401159" class="bulleted-list"><li style="list-style-type:disc"><strong>Ensemble Methods:</strong> Combine multiple models to get a broader view of the data, which can help reduce the impact of false neighborhoods and improve overall accuracy.[<strong>Random Forests</strong> or <strong>Boosting]</strong></li></ul></details></li></ul></details></li></ul><ul id="ffff532a-f6dc-812d-b9df-cb371d5f5a4b" class="toggle"><li><details open=""><summary><strong>Anomaly Detection</strong>:</summary><figure class="block-color-pink_background callout" style="white-space:pre-wrap;display:flex" id="56c48165-2dea-40a7-ab0d-2a54018e0e25"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="ffff532a-f6dc-8169-aa11-f45ebc506e6d" class=""><strong>method to detect outliers or Anomalies, or Novelties</strong></p><p id="c551643a-1453-4f75-8683-06e7d1561a92" class="">Anomaly detection is the process of identifying data points, patterns, or observations that deviate significantly from the majority of the data. </p></div></figure><p id="9ab546be-bb55-473e-a2e3-1b4c96c27dcf" class="">Techniques for Anomaly Detection</p><table id="ffff532a-f6dc-81d0-bb3a-c3aa0db1feb3" class="simple-table"><thead class="simple-table-header"><tr id="ffff532a-f6dc-814b-bf61-f47e4cae6540"><th id="NWYu" class="simple-table-header-color simple-table-header" style="width:284.4666748046875px"></th><th id="GQ]s" class="simple-table-header-color simple-table-header" style="width:204.45834350585938px"><strong>Test/Algorithms</strong></th><th id="PX\I" class="simple-table-header-color simple-table-header" style="width:341.4583435058594px">Definition</th><th id="LAXW" class="simple-table-header-color simple-table-header" style="width:284.4666748046875px">Formula</th><th id="[qc`" class="simple-table-header-color simple-table-header" style="width:284.4666748046875px">Applications</th></tr></thead><tbody><tr id="ffff532a-f6dc-811a-9736-cb0b389a931a"><th id="NWYu" class="simple-table-header-color simple-table-header" style="width:284.4666748046875px"><strong>Statistical Methods</strong></th><td id="GQ]s" class="" style="width:204.45834350585938px">Z Score </td><td id="PX\I" class="" style="width:341.4583435058594px">Measures the number of standard deviations a data point is from the mean.<br/>Indicates how many standard deviations a data point is from the mean of a distribution<br/></td><td id="LAXW" class="" style="width:284.4666748046875px">z=(x−μ)/ σ<br/>​<br/> <br/><br/></td><td id="[qc`" class="" style="width:284.4666748046875px">Data points with z-scores beyond a certain threshold are considered anomalies.</td></tr><tr id="ffff532a-f6dc-814d-b772-d98b88fb9022"><th id="NWYu" class="simple-table-header-color simple-table-header" style="width:284.4666748046875px"></th><td id="GQ]s" class="" style="width:204.45834350585938px">Grubbs Test</td><td id="PX\I" class="" style="width:341.4583435058594px">A statistical test to detect outliers in a univariate dataset.</td><td id="LAXW" class="" style="width:284.4666748046875px"></td><td id="[qc`" class="" style="width:284.4666748046875px">Suitable for small sample sizes and assumes normal distribution.</td></tr><tr id="a8491ee0-ce5f-4542-9d7c-b570802bc6ca"><th id="NWYu" class="simple-table-header-color simple-table-header" style="width:284.4666748046875px">Distance Based </th><td id="GQ]s" class="" style="width:204.45834350585938px">KNN</td><td id="PX\I" class="" style="width:341.4583435058594px">Measures the distance of a data point from its k-nearest neighbors.</td><td id="LAXW" class="" style="width:284.4666748046875px"></td><td id="[qc`" class="" style="width:284.4666748046875px">Data points with significantly larger average distances are considered anomalies.</td></tr><tr id="081b31e7-b38c-4ecf-86b4-d599cac70cad"><th id="NWYu" class="simple-table-header-color simple-table-header" style="width:284.4666748046875px">Machine Learning Based Methods</th><td id="GQ]s" class="" style="width:204.45834350585938px">Isolation Forest</td><td id="PX\I" class="" style="width:341.4583435058594px">Identifies anomalies by isolating data points using isolation trees.</td><td id="LAXW" class="" style="width:284.4666748046875px"></td><td id="[qc`" class="" style="width:284.4666748046875px"></td></tr><tr id="8e21a9b5-2e9e-4eca-b792-d21d2f8a7c72"><th id="NWYu" class="simple-table-header-color simple-table-header" style="width:284.4666748046875px"></th><td id="GQ]s" class="" style="width:204.45834350585938px">One Class SVM</td><td id="PX\I" class="" style="width:341.4583435058594px">One-Class SVM (Support Vector Machine) is a variation of SVM that is used for anomaly detection by learning a decision function that separates the majority of the data from the origin.</td><td id="LAXW" class="" style="width:284.4666748046875px"></td><td id="[qc`" class="" style="width:284.4666748046875px"></td></tr><tr id="ffff532a-f6dc-810c-9cb9-d1fb38814696"><th id="NWYu" class="simple-table-header-color simple-table-header" style="width:284.4666748046875px"></th><td id="GQ]s" class="" style="width:204.45834350585938px">Autoencoders</td><td id="PX\I" class="" style="width:341.4583435058594px">Autoencoders are neural networks designed to learn a compressed representation of data and reconstruct the original input. Anomalies are detected based on reconstruction error.</td><td id="LAXW" class="" style="width:284.4666748046875px"></td><td id="[qc`" class="" style="width:284.4666748046875px"></td></tr></tbody></table><div id="ffff532a-f6dc-8102-b2ee-fa5a49e87df8" class="column-list"><div id="ffff532a-f6dc-8151-a6d6-ced09fb4fcea" style="width:100%" class="column"><p id="ffff532a-f6dc-8146-9116-d409dc3bd2b0" class="block-color-gray_background"><strong><span style="border-bottom:0.05em solid">Isolation Forest </span></strong></p><figure id="ffff532a-f6dc-815d-89d5-f77a27e82ca8" class="image"><a href="illustration-isolation-forest.webp"><img style="width:449.4305725097656px" src="illustration-isolation-forest.webp"/></a></figure><p id="51e3369d-e38f-4d8b-bb95-4e44f7368e91" class=""><strong>Steps:</strong></p><ol type="1" id="ffff532a-f6dc-81ce-aac5-e771267c00e8" class="numbered-list" start="1"><li><strong>Data Preparation:</strong><ul id="ffff532a-f6dc-8182-87f7-fbd82ca5dee2" class="bulleted-list"><li style="list-style-type:disc"><strong>Preprocessing:</strong> Normalize or standardize the data if necessary. Isolation Forest is generally robust to feature scaling but ensuring features are on a similar scale can be beneficial.</li></ul></li></ol><ol type="1" id="8fbe9b6b-3378-41bb-9291-2e646d3200a5" class="numbered-list" start="2"><li><strong>Building Isolation Trees:</strong><ul id="ffff532a-f6dc-810a-b969-c1af95078570" class="bulleted-list"><li style="list-style-type:disc"><strong>Tree Construction:</strong> Create multiple isolation trees (usually hundreds) where each tree is built by randomly selecting a feature and then randomly selecting a split value within the range of that feature.</li></ul><ul id="434f4a47-5701-41e6-be9a-7e11765a9f70" class="bulleted-list"><li style="list-style-type:disc"><strong>Isolation:</strong> The goal is to isolate each data point as quickly as possible. Anomalies tend to be isolated earlier due to their distinctiveness.</li></ul></li></ol><ol type="1" id="a4a9d7ee-05d5-40be-ac0a-653c9a1b75d8" class="numbered-list" start="3"><li><strong>Scoring Anomalies:</strong><ul id="ffff532a-f6dc-8179-a183-fbb7bc132879" class="bulleted-list"><li style="list-style-type:disc"><strong>Path Length:</strong> Measure the average path length required to isolate each data point across all isolation trees.</li></ul><ul id="ffff532a-f6dc-81ea-ba31-e361c43f135e" class="bulleted-list"><li style="list-style-type:disc"><strong>Anomaly Score:</strong> Calculate the anomaly score based on the path length. Data points with shorter path lengths are considered anomalies.</li></ul></li></ol><ol type="1" id="ffff532a-f6dc-818b-81c8-cdc0a6fd9612" class="numbered-list" start="4"><li><strong>Thresholding:</strong><ul id="ffff532a-f6dc-8199-a5a4-deb11afb4246" class="bulleted-list"><li style="list-style-type:disc"><strong>Determine Threshold:</strong> Set a threshold for the anomaly score to classify points as anomalies or normal. This can be done using domain knowledge or cross-validation.</li></ul></li></ol><p id="ffff532a-f6dc-816d-a020-d380d56e7999" class=""><strong>1.3. Advantages:</strong></p><ul id="264530aa-c20a-40b8-a30c-0af748e5af72" class="bulleted-list"><li style="list-style-type:disc"><strong>Efficiency:</strong> Works well with large datasets and high-dimensional data.</li></ul><ul id="ffff532a-f6dc-8169-b989-e88d63deccca" class="bulleted-list"><li style="list-style-type:disc"><strong>Interpretability:</strong> Anomalies are identified based on isolation, which is intuitive.</li></ul><p id="ffff532a-f6dc-81bf-8416-f8b593c9a98e" class=""><strong>1.4. Challenges:</strong></p><ul id="fb5f1d59-b1be-449f-ba85-965c04b3b441" class="bulleted-list"><li style="list-style-type:disc"><strong>Sensitivity to Parameters:</strong> The number of trees and the depth of trees need to be tuned for optimal performance.</li></ul></div><div id="ffff532a-f6dc-8151-a34c-dc2fbe327b4b" style="width:100%" class="column"><p id="ffff532a-f6dc-8157-87cd-d580f8a734c7" class="block-color-teal_background"><strong><span style="border-bottom:0.05em solid">OCSVM</span></strong></p><p id="ffff532a-f6dc-8140-be15-f0e3c2452cbe" class="">
</p><figure id="ffff532a-f6dc-814c-aa27-ebf8e151fd63" class="image" style="text-align:center"><a href="energies-15-08757-g002.webp"><img style="width:240px" src="energies-15-08757-g002.webp"/></a></figure><p id="ffff532a-f6dc-8129-b84e-c744dd4bbeae" class=""><strong>Steps:</strong></p><ol type="1" id="2ab1b2e7-63c1-46dc-8808-c5f23843f3a2" class="numbered-list" start="1"><li><strong>Data Preparation:</strong><ul id="ffff532a-f6dc-81ca-80e4-c6bd2b1af5f7" class="bulleted-list"><li style="list-style-type:disc"><strong>Preprocessing:</strong> Standardize or normalize the features as One-Class SVM is sensitive to feature scaling.</li></ul></li></ol><ol type="1" id="5e871bc1-c551-4476-b57e-ac61aaff9180" class="numbered-list" start="2"><li><strong>Training the Model:</strong><ul id="ffff532a-f6dc-8113-8848-f8d9fe8f44bc" class="bulleted-list"><li style="list-style-type:disc"><strong>Model Training:</strong> Train the One-Class SVM on normal data. The algorithm learns to find a hyperplane that best separates the normal data from the origin.</li></ul><ul id="ffff532a-f6dc-81d9-a7b1-eb459944faee" class="bulleted-list"><li style="list-style-type:disc"><strong>Kernel Choice:</strong> Use a kernel function (e.g., radial basis function (RBF)) to handle non-linearly separable data. The choice of kernel and its parameters can significantly impact performance.</li></ul></li></ol><ol type="1" id="ffff532a-f6dc-810d-999b-dae03499d694" class="numbered-list" start="3"><li><strong>Scoring Anomalies:</strong><ul id="0d0c8c83-e3d8-4e88-bd1b-e5bd174d4322" class="bulleted-list"><li style="list-style-type:disc"><strong>Decision Function:</strong> The decision function provides a score indicating how far a data point is from the learned hyperplane.</li></ul><ul id="ffff532a-f6dc-8147-b765-ddd157945c45" class="bulleted-list"><li style="list-style-type:disc"><strong>Anomaly Score:</strong> Data points falling outside the learned boundary (i.e., those with scores below a certain threshold) are considered anomalies.</li></ul></li></ol><ol type="1" id="8902c540-2f52-45ef-8c2d-d0133575d6cf" class="numbered-list" start="4"><li><strong>Thresholding:</strong><ul id="ffff532a-f6dc-8152-bf7d-c74352f883e3" class="bulleted-list"><li style="list-style-type:disc"><strong>Set Threshold:</strong> Define a threshold to classify data points as anomalies or normal. This can be done through cross-validation or domain knowledge.</li></ul></li></ol><p id="ffff532a-f6dc-8172-914a-c8917b618213" class=""><strong>2.3. Advantages:</strong></p><ul id="49f531fe-e649-49fa-a300-24d1bfe2cce8" class="bulleted-list"><li style="list-style-type:disc"><strong>Flexibility:</strong> Can handle complex, non-linear relationships using appropriate kernels.</li></ul><ul id="fe353381-5477-4b4c-96e9-400d42a4e4d8" class="bulleted-list"><li style="list-style-type:disc"><strong>Effective in High Dimensions:</strong> Performs well with high-dimensional data.</li></ul><p id="ffff532a-f6dc-8187-b2cc-f2cadc7e3cb8" class=""><strong>2.4. Challenges:</strong></p><ul id="0b4e060c-b973-4d82-9cab-669d0290ec50" class="bulleted-list"><li style="list-style-type:disc"><strong>Parameter Tuning:</strong> Requires careful tuning of parameters like the kernel type and regularization term.</li></ul></div><div id="2e5e691e-c7b5-4cfe-90d0-ad061112c9a0" style="width:100%" class="column"><p id="ffff532a-f6dc-81b3-9853-ff61e5b9690a" class="block-color-orange_background"><strong><span style="border-bottom:0.05em solid">Autoencoders</span></strong></p><p id="ffff532a-f6dc-81ee-9026-d53f995f33de" class="">
</p><figure id="ffff532a-f6dc-81aa-8101-ead12aa88c4f" class="image"><a href="DRjpx.png"><img style="width:384px" src="DRjpx.png"/></a></figure><p id="0d20139b-ff9f-4c49-82cf-b8b2c5a03278" class="">
</p><p id="777c2f11-9040-4788-a46a-ad8581507ede" class=""><strong>Steps:</strong></p><ol type="1" id="ffff532a-f6dc-8145-9ad6-d9c1ce7bb292" class="numbered-list" start="1"><li><strong>Data Preparation:</strong><ul id="52146dc2-015b-4647-ba8a-cb81c72543a4" class="bulleted-list"><li style="list-style-type:disc"><strong>Preprocessing:</strong> Normalize or standardize the data. Autoencoders are sensitive to feature scales.</li></ul></li></ol><ol type="1" id="ffff532a-f6dc-812f-86bb-e7483775ce45" class="numbered-list" start="2"><li><strong>Training the Autoencoder:</strong><ul id="ffff532a-f6dc-818f-b160-d565e6e8e69c" class="bulleted-list"><li style="list-style-type:disc"><strong>Network Architecture:</strong> Design an autoencoder with an encoder part that compresses data into a lower-dimensional representation and a decoder part that reconstructs the original input.</li></ul><ul id="ffff532a-f6dc-8118-8534-d928d0998add" class="bulleted-list"><li style="list-style-type:disc"><strong>Loss Function:</strong> Use a loss function like Mean Squared Error (MSE) to measure the reconstruction error during training.</li></ul></li></ol><ol type="1" id="ffff532a-f6dc-8149-b6cc-eb74b5e1de56" class="numbered-list" start="3"><li><strong>Evaluating Reconstruction Error:</strong><ul id="c73384d4-343d-41f2-a882-eb07822660eb" class="bulleted-list"><li style="list-style-type:disc"><strong>Reconstruction:</strong> After training, pass data points through the autoencoder to get the reconstructed output.</li></ul><ul id="ffff532a-f6dc-81a6-9b7c-fe0eba135bef" class="bulleted-list"><li style="list-style-type:disc"><strong>Error Calculation:</strong> Compute the reconstruction error (e.g., MSE between original and reconstructed data).</li></ul></li></ol><ol type="1" id="ffff532a-f6dc-81b8-8eb8-d97652ac3a7e" class="numbered-list" start="4"><li><strong>Scoring Anomalies:</strong><ul id="7078281d-9dd8-4751-9ced-4608df10194c" class="bulleted-list"><li style="list-style-type:disc"><strong>Anomaly Score:</strong> Data points with high reconstruction error are considered anomalies, as they are not well-represented by the autoencoder.</li></ul></li></ol><ol type="1" id="6bdea03c-e938-46db-8e3d-9f94f2190611" class="numbered-list" start="5"><li><strong>Thresholding:</strong><ul id="e9e66281-75e0-42d0-a81c-a5d08c624d9c" class="bulleted-list"><li style="list-style-type:disc"><strong>Set Threshold:</strong> Determine a threshold for the reconstruction error to classify data points as anomalies or normal. This can be set based on statistical properties of the error distribution or using validation data.</li></ul></li></ol><p id="ffff532a-f6dc-8105-a7d1-c8299b399d58" class="">
</p></div></div><p id="ffff532a-f6dc-8110-bf6e-c136ad25fe59" class=""><strong><span style="border-bottom:0.05em solid">Challenges in Anomaly Detection:</span></strong></p><ul id="ffff532a-f6dc-81b2-9361-d15c9de9e78b" class="bulleted-list"><li style="list-style-type:disc"><strong>High Dimensionality:</strong><ul id="8467cab2-4544-42ae-9450-bfdccc509b1e" class="bulleted-list"><li style="list-style-type:circle"><strong>Issue:</strong> Anomalies may be difficult to detect in high-dimensional spaces due to the curse of dimensionality.</li></ul><ul id="ffff532a-f6dc-81a2-aad2-e0de31943db8" class="bulleted-list"><li style="list-style-type:circle"><strong>Solution:</strong> Dimensionality reduction techniques such as Principal Component Analysis (PCA) can help.</li></ul></li></ul><ul id="ffff532a-f6dc-8100-ae94-c188e66ed32d" class="bulleted-list"><li style="list-style-type:disc"><strong>Imbalanced Data:</strong><ul id="80d7e28d-4b5c-40e4-88e7-3f05cd3e1d66" class="bulleted-list"><li style="list-style-type:circle"><strong>Issue:</strong> Anomalies are often rare compared to normal data, leading to imbalanced datasets.</li></ul><ul id="ffff532a-f6dc-815e-a4ae-ce0b9b1c3ad7" class="bulleted-list"><li style="list-style-type:circle"><strong>Solution:</strong> Use techniques designed for imbalanced data, such as resampling or cost-sensitive learning.</li></ul></li></ul><p id="1bfaaa5b-6dc8-4033-801d-58f7ea55c628" class=""><strong><span style="border-bottom:0.05em solid">Applications:</span></strong></p><ul id="ffff532a-f6dc-811f-8aa6-f9ef8ec98a2a" class="bulleted-list"><li style="list-style-type:disc"><strong>Fraud Detection:</strong> Identifying unusual patterns in financial transactions.</li></ul><ul id="ffff532a-f6dc-816b-91c6-d1489b131833" class="bulleted-list"><li style="list-style-type:disc"><strong>Network Security:</strong> Detecting unusual network traffic patterns that may indicate security breaches.</li></ul><ul id="ffff532a-f6dc-8148-a6ac-dd03e66cfff5" class="bulleted-list"><li style="list-style-type:disc"><strong>Industrial Monitoring:</strong> Detecting faults or failures in machinery or equipment.</li></ul><ul id="0087a1c3-6152-4cf9-b34a-002a72736e8c" class="bulleted-list"><li style="list-style-type:disc"><strong>Healthcare:</strong> Identifying abnormal patient health indicators.</li></ul></details></li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0"><mark class="highlight-red">Clustering, low-dimensional representation of high-dimensional data, data compression, (Principal Component Analysis (PCA), SVD, t-SNE, Self Organizing Maps)</mark></summary><div class="indented"><ul id="ffff532a-f6dc-81c9-b8d0-cb53526f04c8" class="toggle"><li><details open=""><summary><strong>Clustering </strong></summary><figure class="block-color-pink_background callout" style="white-space:pre-wrap;display:flex" id="12117383-64d9-45d2-b6ea-8c5b61870023"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="fd906689-0d4c-477a-9443-de7b14ea5a94" class=""><strong>Definition:</strong> Unsupervised Learning technique which is used to group data points based on the similarity or distance </p><p id="ffff532a-f6dc-81c1-b0c9-f75ffbc04755" class=""><strong>Goal </strong>: Partition data into clusters where points in the same cluster are more similar than points in different clusters.</p></div></figure><div id="ffff532a-f6dc-8159-bec2-f82b9baac0a4" class="column-list"><div id="ffff532a-f6dc-81f8-89c8-fe28b1399488" style="width:93.75%" class="column"><p id="ea10e5d4-4f15-4061-8092-d1bd01cce28e" class=""><strong>K Means Clustering </strong></p><ul id="ffff532a-f6dc-810d-b2e7-eb6a73b08434" class="bulleted-list"><li style="list-style-type:disc">Algorithm that partitions data into <em>k</em> clusters.</li></ul><ul id="ffff532a-f6dc-81c1-a38c-f7b86956aca4" class="bulleted-list"><li style="list-style-type:disc">Each cluster is represented by its centroid (mean of points in that cluster).</li></ul><ul id="ffff532a-f6dc-817c-8f8a-ea5a6868b9ee" class="bulleted-list"><li style="list-style-type:disc">Objective: Minimize the sum of squared distances (Eucledean distance)between each point and its assigned centroid.</li></ul><figure class="block-color-pink_background callout" style="white-space:pre-wrap;display:flex" id="ffff532a-f6dc-812b-8373-c6535fabc41c"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="0c59c94e-71ed-42b9-940f-6a711ad7ae6f" class="">Euclidean vs Manhattan Distance </p><figure id="554ae9d3-855e-48b0-916c-6d7bf9679f54" class="image"><a href="june-30-hierarchical-clustering-infograph-for-blog-5.png"><img style="width:445.99609375px" src="june-30-hierarchical-clustering-infograph-for-blog-5.png"/></a></figure></div></figure><p id="d78c0d5c-5c32-4062-ae90-35bbc828d3e2" class="">
</p></div><div id="440ffae5-3545-4eea-96bf-d30552e23565" style="width:100.00000000000003%" class="column"><p id="28e59dc1-2a0b-4153-83ab-f25ff44da9f4" class="">
</p><figure id="ffff532a-f6dc-8129-a0c0-db0aa5aefd39" class="image"><a href="slide32.jpg"><img style="width:553.984375px" src="slide32.jpg"/></a></figure><figure id="ffff532a-f6dc-8112-b476-f75b3049bca3" class="image"><a href="download.png"><img style="width:480px" src="download.png"/></a></figure></div></div><p id="0df50375-cafe-4563-94db-ee517f16ab1a" class="">
</p></details></li></ul><ul id="ffff532a-f6dc-8147-8a3e-ea41c8fc2a44" class="toggle"><li><details open=""><summary><strong>Low dimensional representation of high dimensional data : Dimensionality Reduction</strong></summary><figure class="block-color-pink_background callout" style="white-space:pre-wrap;display:flex" id="9aa7d5af-cbf1-4813-8b1e-477c55de2f22"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="ffff532a-f6dc-811a-82d2-ccaf12e6889d" class=""><strong>Why low dimension is needed :</strong> </p><ul id="ffff532a-f6dc-8168-9f91-c6d637008cff" class="bulleted-list"><li style="list-style-type:disc">Noise Removal ( Outliers)</li></ul><ul id="ab737459-ec57-4f0b-b9e5-55e474f03a84" class="bulleted-list"><li style="list-style-type:disc">Feature Extraction : Extract meaningful/most important  features from the original data</li></ul><ul id="ffff532a-f6dc-8122-9a76-c2b995f1d6d7" class="bulleted-list"><li style="list-style-type:disc">Enhanced Visualization</li></ul><ul id="ffff532a-f6dc-818c-a55a-d9be00438505" class="bulleted-list"><li style="list-style-type:disc">Improve computational efficiency especially for large datasets</li></ul></div></figure><p id="ffff532a-f6dc-814d-9555-e320d754c0aa" class="block-color-blue_background"><em><mark class="highlight-pink"><strong><span style="border-bottom:0.05em solid">Techniques for Dimensionality Reduction</span></strong></mark></em></p><ul id="ffff532a-f6dc-81c1-921c-fb606882129e" class="toggle"><li><details open=""><summary><strong>Principle Component Analysis (PCA)</strong></summary><p id="ffff532a-f6dc-81d5-aac5-f7a7c0e244f8" class="">PCA is a dimensionality reduction technique primarily used to simplify data without losing significant information</p><div id="ef9144f0-ec42-4ed4-a746-84f3394e659f" class="column-list"><div id="7a6bf185-e665-4aeb-87fc-ce092f563890" style="width:100%" class="column"><p id="ffff532a-f6dc-819d-85c2-e2a24504c6b7" class=""><strong>Problem PCA Solves:</strong></p><ol type="1" id="c6c71936-8ee8-4b1a-80b6-3779bb1a92a0" class="numbered-list" start="1"><li><strong>Curse of Dimensionality:</strong> <em>High-dimensional data often suffers from overfitting, noise, and computational inefficiency</em>. PCA reduces dimensionality, mitigating these issues.</li></ol><ol type="1" id="2da23e01-b392-445b-a850-bad016953ac4" class="numbered-list" start="2"><li><strong>Data Visualization:</strong> It makes it easier to visualize and interpret complex data by projecting it onto fewer dimensions.</li></ol><ol type="1" id="ffff532a-f6dc-8190-a5d0-f24f8b359f09" class="numbered-list" start="3"><li><strong>Feature Redundancy:</strong> PCA helps in identifying and removing correlated features, which reduces redundancy and improves model performance. <span style="border-bottom:0.05em solid">“</span><strong><span style="border-bottom:0.05em solid">Redundant Features</span></strong><span style="border-bottom:0.05em solid"> in machine learning refer to features that provide little or no additional information beyond what is already captured by other features”</span></li></ol><figure class="block-color-pink_background callout" style="white-space:pre-wrap;display:flex" id="aae37fcb-d5a0-418c-a59a-a4d72de3bb0e"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="775ddb33-8d4c-4040-8de7-e3feb7e9e827" class=""><strong>Principle of PCA:</strong><br/>The core principle of PCA is to <br/><strong>identify the directions (principal components) along which the variance of the data is maximized</strong>. </p><p id="ffff532a-f6dc-81d1-a529-d5126d15e22f" class="">These <strong>directions are orthogonal </strong>(uncorrelated) and form a new basis for the data.</p></div></figure><p id="ca3a53a8-fd9a-4949-accc-7cfd2560ec97" class=""><strong>Mathematical Solution:</strong></p><ol type="1" id="ffff532a-f6dc-81e4-a110-e94838a5a889" class="numbered-list" start="1"><li><strong>Standardization:</strong> Center the data by subtracting the mean of each feature. If the features are on different scales, standardize them by dividing by their standard deviations [<em><span style="border-bottom:0.05em solid">Center the data , to  ensure that each feature contributes equally to the variance calculation, making PCA effective and unbiased</span></em>] </li></ol><ol type="1" id="0ba51e62-728d-4112-b524-d646ff2598bd" class="numbered-list" start="2"><li><strong>Covariance Matrix:</strong> Compute the covariance matrix of the centered data to capture the variance and correlation between features. [<em><span style="border-bottom:0.05em solid">The covariance matrix captures the variance of each feature and the relationships between features. It provides insight into how features vary together. Also needed to calculate the Principal components(direction along which data varies most]</span></em></li></ol><ol type="1" id="b180328b-e733-4fbf-a4a5-372764d6af0c" class="numbered-list" start="3"><li><strong>Eigenvalue Decomposition:</strong> Perform eigenvalue decomposition on the covariance matrix to find eigenvectors and eigenvalues. The eigenvectors (principal components) represent the directions of maximum variance, and the eigenvalues indicate the amount of variance captured by each principal component. [<em><span style="border-bottom:0.05em solid">Eigenvectors of the covariance matrix represent the directions of maximum variance in the data, Each eigenvector (principal component) is a new axis in the transformed space.][eigenvalue indicates the amount of variance captured by that eigenvector]</span></em></li></ol><ol type="1" id="ffff532a-f6dc-8100-815b-db0f8ba9144c" class="numbered-list" start="4"><li><strong>Projection:</strong> Select the top k eigenvectors (principal components) corresponding to the largest eigenvalues to form a new feature space. Project the original data onto this reduced space.</li></ol><blockquote id="6a65e264-be70-47cb-ab54-6eeb2a076b1d" class="block-color-blue_background"><strong>Summary</strong> : PCA offers significant advantages by simplifying complex data, improving interpretability, reducing computational costs, and enhancing the performance of machine learning models. Its ability to reduce dimensionality while retaining key patterns and structures makes it a valuable tool in data preprocessing and analysis.</blockquote><figure id="ffff532a-f6dc-8191-b053-dc7512ef34fa" class="image"><a href="Principal-Componenent-Analysisi.webp"><img style="width:480px" src="Principal-Componenent-Analysisi.webp"/></a></figure></div><div id="ffff532a-f6dc-814f-9cc4-ed49e58bb369" style="width:68.75%" class="column"><figure id="ffff532a-f6dc-8141-bcfc-f5b2877e09eb" class="image"><a href="image%206.png"><img style="width:686.40625px" src="image%206.png"/></a></figure><p id="68de754b-66cf-40e2-a1ee-204d7db8d3cb" class="">
</p><p id="ffff532a-f6dc-8142-bad6-e211980f1c9e" class=""><strong>Significance of PCA:</strong></p><ol type="1" id="ffff532a-f6dc-81c8-919e-daf4e2e3e70b" class="numbered-list" start="1"><li><strong>Dimensionality Reduction:</strong> By reducing the number of dimensions, PCA helps to simplify models, decrease computational costs, and mitigate overfitting.</li></ol><ol type="1" id="ffff532a-f6dc-81e3-a191-cb9becb9e3a3" class="numbered-list" start="2"><li><strong>Data Compression:</strong> PCA can compress data while preserving significant features, which is useful for storage and speed in processing.</li></ol><ol type="1" id="ffff532a-f6dc-81be-8f13-cf227ff695d7" class="numbered-list" start="3"><li><strong>Noise Reduction:</strong> By focusing on components with the most variance, PCA can reduce the impact of noise and less informative features.</li></ol><figure class="block-color-pink_background callout" style="white-space:pre-wrap;display:flex" id="ffff532a-f6dc-8175-aac0-fa305ee31f8f"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="ffff532a-f6dc-8171-b15a-d6e5c31affa9" class="">Principal components are orthogonal (uncorrelated) because covariance matrix is symmetric.</p><p id="c7552b9d-54de-4fee-87e3-bebd5b0f83dd" class=""> </p><p id="1a58690c-6ace-4079-a49c-8d2cb3776c4d" class="">In PCA, the principal components are eigenvectors of the covariance matrix. </p><p id="ffff532a-f6dc-8121-bda8-eb475404dfc0" class=""><em><span style="border-bottom:0.05em solid">Orthogonality means that these eigenvectors are perpendicular to each other in the high-dimensional space</span></em>. Mathematically, this is represented by the dot product of any two distinct eigenvectors being zero:</p><figure id="abea0e6a-98be-498f-b4f4-63397fb5e8d6" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="bold">u</mi><mo>⋅</mo><mi mathvariant="bold">v</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mathbf{u} \cdot \mathbf{v} = 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4445em;"></span><span class="mord mathbf">u</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4444em;"></span><span class="mord mathbf" style="margin-right:0.01597em;">v</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span></span></span></span></span></div></figure><p id="ffff532a-f6dc-81f6-8413-f18ffab91fe2" class=""> if u and v are distinct eigenvectors.</p></div></figure></div></div></details></li></ul><ul id="78937596-bcaf-4b05-b717-3632bf3715e9" class="toggle"><li><details open=""><summary><strong>Singular Value Decomposition (SVD)</strong></summary><figure class="block-color-pink_background callout" style="white-space:pre-wrap;display:flex" id="e4ff6d19-584c-4031-83e3-d70e2089e79b"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="ffff532a-f6dc-81d4-9bf9-e81fc23fceb8" class="">Singular Value Decomposition (SVD) is a fundamental matrix factorization technique used in linear algebra and machine learning. </p><p id="ffff532a-f6dc-81a4-81c7-f54e0f9dd51f" class="">It decomposes a matrix into three simpler matrices, which can be useful for various applications such as dimensionality reduction, data compression, and solving systems of linear equations</p></div></figure><figure id="2e02ef62-cec4-46bf-ad4d-8678f1812f0d" class="image"><a href="image%207.png"><img style="width:1156.9921875px" src="image%207.png"/></a></figure><figure id="ffff532a-f6dc-8165-8d41-f832fd82f43f"><div class="source"><a href="https://cs.fit.edu/~dmitra/SciComp/Resources/singular-value-decomposition-fast-track-tutorial.pdf">https://cs.fit.edu/~dmitra/SciComp/Resources/singular-value-decomposition-fast-track-tutorial.pdf</a></div></figure><div id="cc028bfd-60ae-42f9-bc43-b4647fbd06da" class="column-list"><div id="db8d3217-be3f-4aa9-852f-af50be8e28c2" style="width:100%" class="column"><p id="e78cb7cd-218c-4609-921e-ab0f0a8f92fa" class=""><strong>Numerical </strong>: <a href="https://www.geeksforgeeks.org/singular-value-decomposition-svd/">https://www.geeksforgeeks.org/singular-value-decomposition-svd/</a> </p><p id="ffff532a-f6dc-81a8-923b-e19519624675" class="">
</p><p id="c52d78de-5940-403f-91de-1cb7b17c534c" class="">In the image (right): </p><p id="be9a6308-20d0-410a-89b0-0e881197e437" class="">The process steps of applying matrix M= UΣV′ on X,</p><ul id="ffff532a-f6dc-810c-bc3a-ccf7bb167093" class="bulleted-list"><li style="list-style-type:disc">Step 1–2 : V′X is a rotation on X.</li></ul><ul id="ffff532a-f6dc-8106-9b64-f617e76332f8" class="bulleted-list"><li style="list-style-type:disc">Step 2–3 : Σ(V′X) is making the stretching.</li></ul><ul id="bac2ab6a-8bb5-4db9-a131-8895125c1507" class="bulleted-list"><li style="list-style-type:disc">Step 3–4: U(ΣV′X) = MX rotates again.</li></ul><p id="9b454a4d-7975-46fb-8f6b-ecd9a10ef457" class="">Steps:</p><ol type="1" id="a61f9b85-8fe9-4ca3-ad68-88979ea018e7" class="numbered-list" start="1"><li>Compute SVD using decomposition</li></ol><ol type="1" id="ffff532a-f6dc-8172-a353-f37ca38f3f0a" class="numbered-list" start="2"><li>Choose no. of components ( singular values k to retain) </li></ol><ol type="1" id="ffff532a-f6dc-81a3-9c2f-fdb7d08e1979" class="numbered-list" start="3"><li>Reduce dimensions based on k values </li></ol><p id="f41f2b82-8fe0-4f60-bfe0-9c0df8766fd3" class="">
</p></div><div id="6db90f17-4617-46c2-8839-df488099e45c" style="width:100%" class="column"><p id="218707d0-2f85-4395-9126-4116a7cd9833" class="">Geometric interpretation of the equation M= UΣV′:</p><figure id="6ad5cb25-f4a6-4601-874a-a0d27ea7e9ab" class="image"><a href="image%208.png"><img style="width:842.421875px" src="image%208.png"/></a></figure><p id="5bf00534-d81c-4bb3-a521-69d5460029eb" class="">
</p></div></div></details></li></ul><ul id="ffff532a-f6dc-81b4-882a-d485c28b5afe" class="toggle"><li><details open=""><summary><strong>t-Distributed Stochastic Neighbor Embedding (t-SNE)</strong></summary><figure class="block-color-pink_background callout" style="white-space:pre-wrap;display:flex" id="9ac81359-c9e2-4546-9444-7df02c0005f1"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="bcfa868c-3800-4590-8755-556c6a40057c" class=""><strong>Objective:</strong></p><ul id="ffff532a-f6dc-81c5-86ed-d8023f67465c" class="bulleted-list"><li style="list-style-type:disc"><strong>Preserve Local Structure:</strong> t-SNE aims to preserve the local structure of the data by maintaining the similarity between data points in the high-dimensional space and their corresponding points in the low-dimensional space.</li></ul><ul id="ffff532a-f6dc-8120-850f-c50cd9fd2a76" class="bulleted-list"><li style="list-style-type:disc">PCA works better on linear data while t-SNE have no such restriction. Whether the data is linear or non-linear t-SNE performs well. </li></ul></div></figure><figure id="ffff532a-f6dc-812a-9b73-fe9148c7ade3" class="image"><a href="image%209.png"><img style="width:2560px" src="image%209.png"/></a></figure><p id="2fcf9cf8-d0bb-4a78-90ba-cbc325f7dd77" class=""><strong><span style="border-bottom:0.05em solid">Core idea of t-SNE :</span></strong></p><ul id="5dc3b4d4-1965-4db0-ad1b-ac43d320e841" class="bulleted-list"><li style="list-style-type:disc">The core idea behind t-SNE is to map high-dimensional data points to a lower-dimensional space, typically two or three dimensions, in a way that <strong>preserves the local relationships between points.</strong></li></ul><ul id="e89d1e4b-5782-4cc1-ab46-9053d434b9bd" class="bulleted-list"><li style="list-style-type:disc"> It achieves this by <strong>measuring the similarity between data points in the high-dimensional space and representing this similarity as probabilities</strong>. </li></ul><ul id="e3bbe76a-831f-4190-873e-0d053415eb55" class="bulleted-list"><li style="list-style-type:disc">Then, it constructs a similar probability distribution in the lower-dimensional space and minimizes the difference between the two distributions using a technique called gradient descent. </li></ul><ul id="ffff532a-f6dc-817b-81cb-d7700601f249" class="bulleted-list"><li style="list-style-type:disc">This process allows t-SNE to effectively capture the local structure of the data, making it particularly useful for visualizing complex datasets and discovering meaningful patterns.</li></ul><p id="9d44f13f-c3b5-4b27-914f-45547277f4f3" class=""><strong><span style="border-bottom:0.05em solid">Computing Similarities:</span></strong></p><p id="ffff532a-f6dc-8127-85cd-c27cd74e7e15" class="">For each data point in the high-dimensional space, we calculate its similarity to every other point [pairwise similarity] using a Gaussian distribution. This similarity is based on the distance between points.</p><p id="ffff532a-f6dc-81bc-aa6f-c318141c4187" class="">Similarly, in the low-dimensional space, we calculate similarities between points using a t-distribution [<strong>Why t </strong>?- to handle the complexity of maintaining local structure, manage extreme distances, mitigate crowding, and provide robustness to outliers.]</p><figure id="c6e32a3c-105d-4c49-91f2-1dbcf5f3c685" class="image" style="text-align:center"><a href="1_dlbJ150SIoG2_2Vq7Dd0XA.png"><img style="width:384px" src="1_dlbJ150SIoG2_2Vq7Dd0XA.png"/></a></figure><p id="931b1acd-fdec-4b4e-bf72-16e7957436bf" class=""><strong><span style="border-bottom:0.05em solid">Cost /Objective Function</span></strong></p><ul id="ffff532a-f6dc-81e8-8956-fdbc42eb1fa8" class="bulleted-list"><li style="list-style-type:disc">We want to minimize the difference between the similarities of points in the high-dimensional space and their counterparts in the low-dimensional space. We measure this difference using the <strong>Kullback-Leibler (KL) divergence</strong>.</li></ul><ul id="a58b6dd4-2043-447f-ad15-c9ea6bb6a6b8" class="bulleted-list"><li style="list-style-type:disc">The KL divergence measures how one probability distribution diverges from another. In our case, it quantifies how different the pairwise similarities are between the high-dimensional and low-dimensional spaces.</li></ul><ul id="8bd029a1-c208-4490-a5bb-4e2128881677" class="bulleted-list"><li style="list-style-type:disc">The Kullback-Leibler (KL) divergence between the high-dimensional probability distribution P and the low-dimensional probability distribution Q is minimized</li></ul><figure id="a091d4bc-a65f-4463-acc0-2a6fae799942" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>KL</mtext><mo stretchy="false">(</mo><mi>P</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>Q</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></munder><msub><mi>P</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi>log</mi><mo>⁡</mo><mfrac><msub><mi>P</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>Q</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mfrac></mrow><annotation encoding="application/x-tex">\text{KL}(P || Q) = \sum_{i,j} P_{ij} \log \frac{P_{ij}}{Q_{ij}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">KL</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord">∣∣</span><span class="mord mathnormal">Q</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.7741em;vertical-align:-1.4138em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4138em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3603em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9721em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div></figure><p id="ffff532a-f6dc-8183-b641-f3e0866f7ed8" class="">
</p></details></li></ul><ul id="37e526f4-5ea3-4f09-b80a-9308b86f5c8a" class="toggle"><li><details open=""><summary><strong>Self-Organizing Maps (SOMs)</strong></summary><figure class="block-color-pink_background callout" style="white-space:pre-wrap;display:flex" id="51eb5ad2-75f6-46fb-8a01-0de99482b2dd"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="b1e69ac5-7ca6-4c27-ad79-9b4252312c3a" class="">SOMs are a type of unsupervised neural network used for dimensionality reduction, clustering, and visualization of high-dimensional data. </p><p id="523889d2-128a-4046-900a-b1a9799245ef" class="">Also called Kohonen Map.</p><p id="7420a20f-eb59-4772-ad51-c25af0da10b7" class="">SOMs help in understanding the structure and relationships within data by mapping it onto a lower-dimensional grid.</p></div></figure><div id="ffff532a-f6dc-81a1-a1bd-f7c8f624aeaf" class="column-list"><div id="ffff532a-f6dc-81de-b213-ce1ae5ef0251" style="width:100%" class="column"><p id="62b9e204-532d-4026-b46a-007aac4e646a" class=""><strong>SOMs use an unsupervised learning algorithm that involves </strong>the following steps:</p><p id="ffff532a-f6dc-81e3-82cd-cbd6ff6bad49" class=""><strong>Initialization:</strong> Randomly initialize the weight vectors of the neurons.</p><p id="fd3a59ea-4617-4552-a0f3-6681bd73e198" class=""><strong>Selection:</strong> For each input data vector, identify the Best Matching Unit (BMU), which is the neuron whose weight vector is closest to the input vector. This is usually done using Euclidean distance:</p><figure id="ffff532a-f6dc-818a-860a-f9aa8ee2e821" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Distance</mtext><mo>=</mo><msqrt><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>w</mi><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow></msqrt></mrow><annotation encoding="application/x-tex">\text{Distance} = \sqrt{\sum_{i=1}^n (x_i - w_i)^2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord text"><span class="mord">Distance</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.1568em;vertical-align:-1.2777em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8791em;"><span class="svg-align" style="top:-5.1168em;"><span class="pstrut" style="height:5.1168em;"></span><span class="mord" style="padding-left:1.056em;"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6514em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7401em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span><span style="top:-3.8391em;"><span class="pstrut" style="height:5.1168em;"></span><span class="hide-tail" style="min-width:0.742em;height:3.1968em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='3.1968em' viewBox='0 0 400000 3196' preserveAspectRatio='xMinYMin slice'><path d='M702 80H40000040
H742v3062l-4 4-4 4c-.667.7 -2 1.5-4 2.5s-4.167 1.833-6.5 2.5-5.5 1-9.5 1
h-12l-28-84c-16.667-52-96.667 -294.333-240-727l-212 -643 -85 170
c-4-3.333-8.333-7.667-13 -13l-13-13l77-155 77-156c66 199.333 139 419.667
219 661 l218 661zM702 80H400000v40H742z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span></span></span></span></span></div></figure><p id="3e83925d-d30b-44f5-baa6-bb4c99c4cb15" class=""><strong>Update:</strong> Update the weight vector of the BMU and its neighboring neurons to be more similar to the input vector. The update rule is:</p><figure id="ffff532a-f6dc-81d1-ae17-d66e205ba30f" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mo>=</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>+</mo><mi>α</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>⋅</mo><msub><mi>h</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>⋅</mo><mo stretchy="false">(</mo><mi>x</mi><mo>−</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">w_{i}(t+1) = w_{i}(t) + \alpha(t) \cdot h_{i,j}(t) \cdot (x - w_{i}(t))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">))</span></span></span></span></span></div></figure><p id="ffff532a-f6dc-81f3-a228-c7af17479919" class="">where:</p><ul id="ffff532a-f6dc-81e3-90bf-c9e64cc60625" class="bulleted-list"><li style="list-style-type:disc">wi(t) is the weight vector of the i-th neuron at time t,</li></ul><ul id="ffff532a-f6dc-8130-8b09-dc77d33fafb1" class="bulleted-list"><li style="list-style-type:disc">α(t)is the learning rate,</li></ul><ul id="ffff532a-f6dc-8177-9ecb-e274a4b0bcb7" class="bulleted-list"><li style="list-style-type:disc">h_{i,j}(t) is the neighborhood function,</li></ul><ul id="ffff532a-f6dc-812b-9a7c-ec397fdfb396" class="bulleted-list"><li style="list-style-type:disc">xis the input vector.</li></ul><p id="ffff532a-f6dc-81da-90ef-e0081e95ad95" class=""><strong>Convergence:</strong> Repeat the process for multiple epochs, gradually decreasing the learning rate and neighborhood size to refine the map</p></div><div id="ffff532a-f6dc-81d5-b3fb-cac153357c1a" style="width:100%" class="column"><p id="ffff532a-f6dc-8139-a1dd-ddc6ef0845ac" class="">
</p><figure id="d54dde1f-74a9-44bb-a2e1-fc859b6e735f" class="image" style="text-align:center"><a href="1_QG7afWQKjY3IpezhNQMzBg.webp"><img style="width:497.1666564941406px" src="1_QG7afWQKjY3IpezhNQMzBg.webp"/></a></figure><p id="ffff532a-f6dc-8134-ae18-ef9b9d9b3bed" class=""><a href="https://www.youtube.com/watch?v=_IRcxgG0FL4">https://www.youtube.com/watch?v=_IRcxgG0FL4</a></p><p id="ffff532a-f6dc-8107-8abc-dc9bc6953e17" class=""><a href="https://chatgpt.com/share/9c8f5763-072d-4909-8990-db4eba70e78c">https://chatgpt.com/share/9c8f5763-072d-4909-8990-db4eba70e78c</a> </p><p id="25f5293b-0ed4-41ce-8902-8093c02e439f" class=""><strong>Advantages:</strong></p><ul id="92428bca-403e-4039-99c3-7cf1ee375c29" class="bulleted-list"><li style="list-style-type:disc">Unsupervised learning method that does not require labeled data.</li></ul><ul id="ffff532a-f6dc-81df-89f3-efef5f3faf70" class="bulleted-list"><li style="list-style-type:disc">Capable of handling high-dimensional data.</li></ul><ul id="573fddc8-3030-4ead-aa09-5e3f2f9613ac" class="bulleted-list"><li style="list-style-type:disc">Provides a clear visual representation of data clusters and relationships</li></ul><p id="ffff532a-f6dc-81b3-8068-c9d5f6c23297" class="">
</p></div></div></details></li></ul></details></li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0">Communication over a noisy channel (basic types of noisy channels, information conveyed by a channel, the noisy-channel coding theorem)</summary><div class="indented"><p id="ffff532a-f6dc-81c6-a091-fa134fc7c09c" class="">Relevant Presentation :  <a href="https://homepages.dcc.ufmg.br/~msalvim/courses/infotheory/L07_CommunicationOverANoisyChannel%5Bstill%5D.pdf">https://homepages.dcc.ufmg.br/~msalvim/courses/infotheory/L07_CommunicationOverANoisyChannel[still].pdf</a></p><p id="ffff532a-f6dc-8136-bae5-c522f59690ef" class="">The key elements in communication over a noisy channel include:</p><figure id="68503bd7-b51c-4c56-b9a6-783b6c683a2d" class="image" style="text-align:center"><a href="information_channel.png"><img style="width:624px" src="information_channel.png"/></a></figure><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="f849c0a3-ee9c-4f20-adb4-b8863f149e3b"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="04adbc6c-83b1-4538-8431-55e810fc415c" class=""><strong>Source</strong>: The entity that generates the information.</p><p id="c94776c3-b4a7-4a76-8c93-cad70e5d25d9" class=""><strong>Encoder</strong>: Converts the information into a signal suitable for transmission.</p><p id="8b134c82-7ed9-428e-b03c-7cdab558289b" class=""><strong>Channel</strong>: The medium through which the signal is transmitted. It may introduce noise, causing errors.</p><p id="ffff532a-f6dc-81cb-bd25-dee20c4dc1a0" class=""><strong>Decoder</strong>: Receives and processes the noisy signal to recover the transmitted information.</p><p id="52994182-c344-4866-abd6-e54219ff9ff1" class=""><strong>Receiver</strong>: The entity that ultimately receives the decoded information.</p></div></figure><ul id="9d075b4d-7091-4c77-b98a-ad0c97ffc6a3" class="toggle"><li><details open=""><summary><strong>Types of Noisy Channels</strong>:</summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="ffff532a-f6dc-8140-85d8-ec6ee3a59c53"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="ffff532a-f6dc-81e0-858c-e997aa9ce347" class="">For a <strong>discrete memoryless channel (DMC)  Q</strong> is a <mark class="highlight-gray_background">triple </mark><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>A</mi><mi>X</mi></msub><mo separator="true">,</mo><msub><mi>A</mi><mi>Y</mi></msub><mo separator="true">,</mo><msub><mi>P</mi><mrow><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(A_X , A_Y ,P_{(Y|X)} )</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1052em;vertical-align:-0.3552em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em;">X</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">Y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.22222em;">Y</span><span class="mord mtight">∣</span><span class="mord mathnormal mtight" style="margin-right:0.07847em;">X</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span><span>﻿</span></span><br/>where<br/><strong><br/><br/></strong><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mi>X</mi></msub></mrow><annotation encoding="application/x-tex">A_X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em;">X</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span><strong>is the input alphabet for the channel,<br/><br/></strong><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mi>Y</mi></msub></mrow><annotation encoding="application/x-tex">A_Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">Y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span><strong> is the output alphabet for the channel, </strong>and<strong><br/><br/></strong><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo></mrow></msub></mrow><annotation encoding="application/x-tex">P_{(Y|X)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0385em;vertical-align:-0.3552em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.22222em;">Y</span><span class="mord mtight">∣</span><span class="mord mathnormal mtight" style="margin-right:0.07847em;">X</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span><strong> is a family of conditional probability distributions </strong>p(y | x) indicating the probability of the channel producing output y ∈ AY when the input is x ∈ AX .</p><p id="e807b4a8-dc4a-447b-8db7-e97983fbb2d3" class=""><strong><span style="border-bottom:0.05em solid">For a channel Q, the set of distributions </span></strong><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{Y|X}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0385em;vertical-align:-0.3552em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">Y</span><span class="mord mtight">∣</span><span class="mord mathnormal mtight" style="margin-right:0.07847em;">X</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span><strong><span style="border-bottom:0.05em solid"> is called the transition distributions or the channel matrix.</span></strong></p></div></figure><p id="9297831d-c8ef-4bfb-b356-b2a686b524ad" class="">
</p><p id="ffff532a-f6dc-818c-9cb7-e1c12932616d" class="block-color-blue_background"><strong>Binary Symmetric Channel</strong></p><div id="ffff532a-f6dc-8122-abe7-f53864539b22" class="column-list"><div id="86c06b96-1101-4814-a987-48322dabd18c" style="width:100%" class="column"><ul id="10b46cc2-84c9-4d93-8e30-b2b0162f560e" class="bulleted-list"><li style="list-style-type:disc">In this model, a transmitter wishes to <strong>send a bit (a zero or a one), and the receiver receives a bit</strong>.</li></ul><ul id="ffff532a-f6dc-81ef-8a6e-f3cb06978155" class="bulleted-list"><li style="list-style-type:disc">It is assumed that the bit is <em>usually</em> transmitted correctly, but that it will be &quot;<strong>flipped</strong>&quot; with a small probability (the &quot;crossover probability&quot;).</li></ul><ul id="ffff532a-f6dc-8158-bde7-f9a41059afb9" class="bulleted-list"><li style="list-style-type:disc">Each transmitted bit has a probability <em>f</em> of being flipped (i.e., turned into the opposite bit) during transmission.</li></ul><ul id="ac2c1db2-80ad-43be-847b-d42dd320d90e" class="bulleted-list"><li style="list-style-type:disc">The probability of correctly receiving the transmitted bit is <code>1 - f</code>.<br/><br/><br/><br/></li></ul></div><div id="ffff532a-f6dc-810b-bb71-dac8a2a3d9da" style="width:100%" class="column"><figure id="ffff532a-f6dc-8113-bdc9-e2450d4b38b8" class="image"><a href="image%2010.png"><img style="width:432px" src="image%2010.png"/></a></figure></div></div><p id="10e1e3dd-74d0-4a95-81a2-0e56ad155f68" class="block-color-blue_background"><strong>Binary Erasure Channel</strong></p><div id="ffff532a-f6dc-81ff-9e23-e73e5fedaba1" class="column-list"><div id="d10d404a-87e6-4a42-8fa9-45b4bf8fbb67" style="width:100%" class="column"><ul id="c984c9c0-af32-477a-852b-8e529c844108" class="bulleted-list"><li style="list-style-type:disc"><strong>Binary erasure channel</strong> (<strong>BEC</strong>) is a <a href="https://en.wikipedia.org/wiki/Communications_channel">communications channel</a> model</li></ul><ul id="ffff532a-f6dc-8165-b89e-f8dda0e02574" class="bulleted-list"><li style="list-style-type:disc">The analog of the binary symmetric channel in which <strong>some bits are lost (rather than corrupted)</strong> is the binary erasure channel.</li></ul><ul id="ffff532a-f6dc-810f-980d-fc67a994ad65" class="bulleted-list"><li style="list-style-type:disc">In this channel, <strong>a fraction α of the bits are erased</strong>. T<strong>he receiver knows which bits have been erased</strong>. The binary erasure channel has <strong>two inputs and three outputs.</strong></li></ul><ul id="52af82c9-13e8-4398-ad59-1558f0f84f17" class="bulleted-list"><li style="list-style-type:disc">The output can thus be <code>0</code>, <code>1</code>, or an erasure symbol <em>?</em><br/><br/><br/></li></ul></div><div id="92f13b49-24e3-4b75-95ea-d001b14f2f19" style="width:100%" class="column"><figure id="9e06646c-5854-4084-a995-9413fc3eb2bf" class="image"><a href="image%2011.png"><img style="width:436.9921875px" src="image%2011.png"/></a></figure></div></div><p id="b7756319-5d5d-4343-b17a-77405cb1c84d" class="block-color-blue_background"><strong>Binary Asymmetric Channel</strong></p><div id="0dfda328-a64f-4515-ad08-01702bdc633d" class="column-list"><div id="de5a26c2-4f7f-41f4-9ef8-429a636f5423" style="width:100%" class="column"><ul id="ffff532a-f6dc-8199-9f95-ec1f38e8c34c" class="bulleted-list"><li style="list-style-type:disc">In the Binary Asymmetric Channel, the probability of flipping a bit depends on whether the input is <code>0</code> or <code>1</code></li></ul><ul id="ffff532a-f6dc-81ec-a333-e3a53f835cb2" class="bulleted-list"><li style="list-style-type:disc">If the input is <code>0</code>, it flips with probability <code>p</code>.</li></ul><ul id="ffff532a-f6dc-81b3-a5e4-d382a088bf79" class="bulleted-list"><li style="list-style-type:disc">If the input is <code>1</code>, it flips with a different probability <code>q</code>.</li></ul><p id="85d42967-d413-4eb1-96a0-445589e4bbd5" class="">
</p></div><div id="ffff532a-f6dc-8115-bc29-fc2a0c595d5b" style="width:100%" class="column"><ul id="ffff532a-f6dc-8130-b916-e93a6be751c5" class="bulleted-list"><li style="list-style-type:disc">This leads to an <strong>asymmetric transition matrix</strong>:[1−pq​p1−q​]<figure id="f50d3ca0-0f6f-44ee-bb8b-16720de5e550" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>1</mn><mo>−</mo><mi>p</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>p</mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>q</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>1</mn><mo>−</mo><mi>q</mi></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><annotation encoding="application/x-tex">\begin{bmatrix}
1 - p &amp; p \\
q &amp; 1 - q
\end{bmatrix}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.4em;vertical-align:-0.95em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">[</span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">p</span></span></span><span style="top:-2.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.95em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">p</span></span></span><span style="top:-2.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.95em;"><span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">]</span></span></span></span></span></span></span></div></figure></li></ul></div></div><p id="ffff532a-f6dc-8173-867d-c784e0d264b4" class="block-color-blue_background"><strong>Binary Asymmetric Channel</strong></p><div id="ffff532a-f6dc-8107-8a6f-fd290b17eca3" class="column-list"><div id="e3d1dffb-a830-4214-8c9f-39f59e6c913e" style="width:100%" class="column"><p id="eae87e98-c79f-4e34-9d23-bdc82b351662" class="">A Z-channel is a channel with binary input and binary output,</p><p id="1199ea2a-51ce-4a63-945d-52e9fe588879" class="">A 0 bit is transmitted correctly<br/>A <br/><code>1</code> can be received as <code>0</code> with a certain probability <code>p</code>.<br/><br/><br/></p><p id="ffff532a-f6dc-819e-bf04-eb031de32d50" class="">
</p></div><div id="df4dcbea-2eee-40fb-acde-6ced02b0c6fd" style="width:100%" class="column"><figure id="ffff532a-f6dc-818d-843c-f5cedf7456e3" class="image"><a href="image%2012.png"><img style="width:407.94921875px" src="image%2012.png"/></a></figure></div></div></details></li></ul><ul id="ffff532a-f6dc-814a-9034-d7b718ac5232" class="toggle"><li><details open=""><summary><strong>Information Conveyed by a Channel</strong>:</summary><p id="ffff532a-f6dc-8172-84c5-ddef8e1e6329" class="">The key metric for understanding communication over a noisy channel is <strong>mutual information.</strong></p><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="ebc6cba0-545c-47f7-9c9c-76e4dab529d7"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="ffff532a-f6dc-8127-a402-d0655d33c4fd" class="">It quantifies the <strong>amount of information that is shared between the input and output of the channel.</strong></p><p id="ffff532a-f6dc-812f-8368-ffd5ce274993" class="">For a channel with input XXX and output YYY, mutual information is given by:</p><figure id="16a28193-ffc1-463a-b4fa-9d924b4e8cca" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>I</mi><mo stretchy="false">(</mo><mi>X</mi><mo separator="true">;</mo><mi>Y</mi><mo stretchy="false">)</mo><mo>=</mo><mi>H</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>−</mo><mi>H</mi><mo stretchy="false">(</mo><mi>X</mi><mi mathvariant="normal">∣</mi><mi>Y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">I(X; Y) = H(X) - H(X|Y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mclose">)</span></span></span></span></span></div></figure><figure id="08264bc6-5f5b-4da4-834a-9660eb615b49" class="image" style="text-align:center"><a href="image%2013.png"><img style="width:457px" src="image%2013.png"/></a></figure><p id="1003bac1-c516-4b02-9cac-bc37b269bd87" class="">Where:</p><ul id="ffff532a-f6dc-81b6-b4c7-c7f85f90ad25" class="bulleted-list"><li style="list-style-type:disc">H(X) is the <strong>entropy</strong> of the input.</li></ul><ul id="ffff532a-f6dc-811e-bc83-e646545495bb" class="bulleted-list"><li style="list-style-type:disc">H(X∣Y) is the <strong>conditional entropy</strong>, representing the uncertainty remaining about X given Y.</li></ul><ul id="e72bb97d-90b6-4dda-8394-1dad4e76ea9d" class="bulleted-list"><li style="list-style-type:disc">I(X;Y) - ; here represents that mutual information measures the <strong>shared information </strong>between X and Y</li></ul><figure id="ffff532a-f6dc-81e4-8063-e62c02fc774d" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>I</mi><mo stretchy="false">(</mo><mi>X</mi><mo separator="true">;</mo><mi>Y</mi><mo stretchy="false">)</mo><mo>=</mo><mi>I</mi><mo stretchy="false">(</mo><mi>Y</mi><mo separator="true">;</mo><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>H</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>−</mo><mi>H</mi><mo stretchy="false">(</mo><mi>X</mi><mi mathvariant="normal">∣</mi><mi>Y</mi><mo stretchy="false">)</mo><mo>=</mo><mi>H</mi><mo stretchy="false">(</mo><mi>Y</mi><mo stretchy="false">)</mo><mo>−</mo><mi>H</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">I(X; Y) = I(Y;X)= H(X) - H(X|Y) = H(Y)-H(Y|X) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">)</span></span></span></span></span></div></figure><figure id="860af344-8450-4fd8-92ec-6a347b14d152" class="image"><a href="image%2014.png"><img style="width:1296px" src="image%2014.png"/></a></figure><p id="ffff532a-f6dc-8145-9d6b-f302b07fc013" class="">Where:</p><ul id="ffff532a-f6dc-81ca-8c9a-f0b85ad03a93" class="bulleted-list"><li style="list-style-type:disc"><em>p(x,y) </em>is the joint probability of input X=x and output Y=y.</li></ul><ul id="cd7d9e49-6612-466e-9f4a-55a507ac1d21" class="bulleted-list"><li style="list-style-type:disc"><em>p(x)</em> is the probability distribution of the input symbols.</li></ul><ul id="3f5f68fa-a484-426b-ab03-06dcf6cb29ff" class="bulleted-list"><li style="list-style-type:disc"><em>p(y)</em> is the probability distribution of the output symbols.</li></ul><p id="952dded7-2c7a-4752-add3-3c6604310919" class=""><strong>Mutual Informatic is symmetric I(X;Y)=I(Y;X) and on negative I(X|Y) ≥0</strong></p></div></figure><ul id="e3772de6-39e4-4e49-a2ca-85ef27a0a9fc" class="bulleted-list"><li style="list-style-type:disc">Channel Capacity<figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="6bce3203-d752-49f4-b15c-a3daf685fd8d"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="ffff532a-f6dc-81fd-8c98-f1dcd9b08e53" class=""><strong>For every discrete memoryless channel, the channel capacity, Channel Capacity</strong> is the maximum amount of information that can be reliably transmitted across the channel per use of the channel.</p></div></figure><p id="ffff532a-f6dc-817b-acb9-e4893cf37732" class="">For a given DMC channel, the <strong>channel capacity C of channel Q</strong>  is defined as:</p><figure id="ffff532a-f6dc-81ae-80f8-c28a03cc9f42" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>C</mi><mo stretchy="false">(</mo><mi>Q</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mrow><mi>max</mi><mo>⁡</mo></mrow><mrow><msub><mi>P</mi><mi>X</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></munder><mi>I</mi><mo stretchy="false">(</mo><mi>X</mi><mo separator="true">;</mo><mi>Y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">C(Q) = \max_{P_X(x)} I(X; Y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.716em;vertical-align:-0.966em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em;"><span style="top:-2.309em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567em;margin-left:-0.1389em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em;">X</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1433em;"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop">max</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.966em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mclose">)</span></span></span></span></span></div></figure><p id="ffff532a-f6dc-8193-b4e5-e81957f6f8e6" class="">The <strong>maximization </strong>is performed <strong>over all possible input distributions </strong><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mi>X</mi></msub></mrow><annotation encoding="application/x-tex">P_X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em;">X</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span></p></li></ul><ul id="ffff532a-f6dc-8101-b0c5-ffc3ece0c6e9" class="toggle"><li><details open=""><summary><strong>Noisy-Channel Coding Theorem</strong>: </summary><p id="ffff532a-f6dc-81c2-aca3-e705c8a55358" class=""><strong>Explanation of the theorem and its implications.</strong><div class="indented"><div id="54a45ba7-dbb2-4a82-bfb2-d400fd833449" class="column-list"><div id="ffff532a-f6dc-81e2-98de-e7463dce266a" style="width:100%" class="column"><p id="3fa0bc74-01af-48c7-8d41-ed462bc001b1" class=""><em>R is Transmission Rate of input steam and C is the channel Capacity</em></p><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="ffff532a-f6dc-8105-9de6-d321db8e74b1"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="a64059ff-eaf8-4c91-84ae-5e7e6878e299" class="">The Shannon theorem states that <strong>given a noisy channel with channel capacity C and information transmitted at a rate R</strong>, </p><p id="ffff532a-f6dc-81fe-bdd2-c52795a43479" class=""> If,  <strong> R&lt;C</strong> </p><p id="854f610c-fba4-46a3-9cf5-71dad9fdbcd6" class=""><strong>There exist codes that allow the probability of error at the receiver to be made arbitrarily small.  Theoretically, it is possible to transmit information nearly without error at any rate below a limiting rate, C.</strong></p><p id="ffff532a-f6dc-8119-b58b-d69f114af5f1" class="">Conversely if,  <strong>R&lt;C, </strong></p><p id="de281a0e-588c-41c1-a322-481a852ee48a" class=""><strong>An arbitrarily small probability of error is not achievable.</strong></p></div></figure><blockquote id="ffff532a-f6dc-8173-92c2-d7170b0da751" class="">The Noisy-Channel Coding Theorem <strong>ensures there is a always a way of encoding data to transmit it through a noisy channel at a rate that achieves capacity</strong></blockquote><p id="ffff532a-f6dc-81e4-abb0-ea065de27901" class="">
</p><p id="ffff532a-f6dc-81bf-b0ea-ed59689c6d7a" class="">
</p><p id="ffff532a-f6dc-8167-aec7-ff95df071deb" class="">
</p></div><div id="0ebd2470-f319-4036-9740-b96577d61124" style="width:100%" class="column"><figure id="ffff532a-f6dc-81ad-a045-cab3e7daea2c" class="image"><a href="image%2015.png"><img style="width:528px" src="image%2015.png"/></a></figure><figure id="266f2fe6-b18e-42dd-8c49-b3d1c3bfc737" class="image"><a href="image%2016.png"><img style="width:528px" src="image%2016.png"/></a></figure></div></div></div></p></details></li></ul></details></li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0">Coding theory, Huffman coding, concepts of arithmetic coding, Hamming codes</summary><div class="indented"><ul id="ffff532a-f6dc-81a1-9f5c-c27b677600f1" class="toggle"><li><details open=""><summary><strong>Coding Theory</strong></summary><p id="ffff532a-f6dc-8125-8fa2-fe92ccb2e585" class="">Coding theory deals with designing codes that allow the transmission of information efficiently and with minimal errors. Codes are used to represent data in a form suitable for storage or transmission over a communication channel.</p><p id="ffff532a-f6dc-8121-af7b-ed89360b698b" class=""><span style="border-bottom:0.05em solid">Source Coding</span> : Focuses on <strong>reducing redundancy</strong> in the data, compressing the information to represent it in fewer bits. [ e.g., Huffman coding, Arithmetic coding]</p><p id="098c511f-f2ac-4701-a13b-bcfc4ba504f8" class=""><span style="border-bottom:0.05em solid">Channel Coding</span> :Designed to <strong>protect data from errors</strong> that may occur during <strong>transmission or storage</strong> by introducing <strong>redundancy</strong>.[Hamming codes]</p><table id="d8bd410a-0597-43e0-a3de-4d62ab76cce2" class="simple-table"><tbody><tr id="8f6c943e-15fb-49aa-872e-54c9d289ab60"><td id="J?sG" class="" style="width:474.1111246744792px"><strong>Aspect</strong></td><td id="pg{N" class="" style="width:474.1111246744792px"><strong>Source Coding</strong></td><td id=";@JF" class="" style="width:474.1111246744792px"><strong>Channel Coding</strong></td></tr><tr id="7f1d356e-5a8c-4c1f-abee-b9f7f21f9522"><td id="J?sG" class="" style="width:474.1111246744792px"><strong>Purpose</strong></td><td id="pg{N" class="" style="width:474.1111246744792px">Reduce the size of the data by removing redundancy</td><td id=";@JF" class="" style="width:474.1111246744792px">Protect data from errors during transmission by adding redundancy</td></tr><tr id="ffff532a-f6dc-8122-b54a-ed0a8a524c41"><td id="J?sG" class="" style="width:474.1111246744792px"><strong>Focus</strong></td><td id="pg{N" class="" style="width:474.1111246744792px">Compression and efficiency</td><td id=";@JF" class="" style="width:474.1111246744792px">Error detection and correction, reliability</td></tr><tr id="ffff532a-f6dc-8136-9c86-ee00beae9779"><td id="J?sG" class="" style="width:474.1111246744792px"><strong>Type of Redundancy</strong></td><td id="pg{N" class="" style="width:474.1111246744792px">Removes redundancy to compress data</td><td id=";@JF" class="" style="width:474.1111246744792px">Adds redundancy to detect and correct errors</td></tr><tr id="ffff532a-f6dc-81f1-b451-f2a48632830d"><td id="J?sG" class="" style="width:474.1111246744792px"><strong>Output</strong></td><td id="pg{N" class="" style="width:474.1111246744792px">A compressed version of the original data</td><td id=";@JF" class="" style="width:474.1111246744792px">A redundant version of the original data that includes error-correcting bits</td></tr><tr id="0a13ee98-e9e6-4e05-94ef-45372650f990"><td id="J?sG" class="" style="width:474.1111246744792px"><strong>Typical Use</strong></td><td id="pg{N" class="" style="width:474.1111246744792px">Data storage and communication</td><td id=";@JF" class="" style="width:474.1111246744792px">Communication over noisy or unreliable channels</td></tr><tr id="ffff532a-f6dc-81c5-8088-e1b4068dce42"><td id="J?sG" class="" style="width:474.1111246744792px"><strong>Examples</strong></td><td id="pg{N" class="" style="width:474.1111246744792px">Huffman coding, Arithmetic coding, JPEG</td><td id=";@JF" class="" style="width:474.1111246744792px">Hamming codes, Reed-</td></tr></tbody></table></details></li></ul><ul id="3de0d7ca-943a-452b-ad9b-b341970326f2" class="toggle"><li><details open=""><summary><strong>Huffman Coding</strong></summary><figure id="083fa5fa-b43c-460b-8e27-7da5e8e307af"><div class="source"><a href="Note_6._Sep_2024.pdf">https://prod-files-secure.s3.us-west-2.amazonaws.com/cc9fb959-9635-4863-80b9-5a928f55ffb0/aecf95a8-d1f2-41b9-9a36-11c2d8eea262/Note_6._Sep_2024.pdf</a></div></figure><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="8d138209-a882-4496-9a81-5462dc417af4"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="ffff532a-f6dc-8176-950f-cf86f9494039" class=""><strong>Huffman coding is a lossless data compression algorithm.</strong></p><p id="ffff532a-f6dc-819e-898b-d73a4bd1db67" class="">The idea is to <span style="border-bottom:0.05em solid">assign variable-length codes</span> to input characters, <span style="border-bottom:0.05em solid">lengths of the assigned codes are based on the frequencies of corresponding characters. </span></p><p id="ffff532a-f6dc-81f5-af22-fb098262a8a9" class="">Lossless - data is compressed while sending , but it remains intact and can be fully recovered upon decompression, making the process &quot;lossless.</p></div></figure><div id="e75de79d-202c-4cc9-b206-724981977c4f" class="column-list"><div id="ffff532a-f6dc-8101-afca-d724eece43e4" style="width:100%" class="column"><p id="05b4ca10-7a6e-4777-aa3a-d2614005cc3a" class=""><strong>Steps to Huffman Coding</strong>:</p><ol type="1" id="ffff532a-f6dc-81d6-bfca-ea819dc376f5" class="numbered-list" start="1"><li><strong>Calculate the frequency</strong> of each symbol in the input data.</li></ol><ol type="1" id="ffff532a-f6dc-81d3-ba56-d1f943b0143b" class="numbered-list" start="2"><li><strong>Build a priority queue</strong> (or min-heap) from the symbols based on their frequencies.</li></ol><ol type="1" id="db14513e-52eb-4339-aa24-fc8768a9c229" class="numbered-list" start="3"><li><strong>Merge nodes</strong>: Repeatedly merge the two nodes with the lowest frequencies, creating a binary tree.</li></ol><ol type="1" id="185458aa-3db1-4659-aba6-ecb7d5aa4004" class="numbered-list" start="4"><li><strong>Assign binary codes</strong> to the symbols:<ul id="3f61f173-0def-4946-8720-09b36fe5525c" class="bulleted-list"><li style="list-style-type:disc">Traverse the binary tree and assign <code>0</code> for left branches and <code>1</code> for right branches.</li></ul><ul id="ffff532a-f6dc-814a-9ebe-e8fbab621801" class="bulleted-list"><li style="list-style-type:disc">Symbols at the leaf nodes are encoded using the binary path from the root.</li></ul></li></ol><p id="71613f4d-dd8f-47f2-9c91-d4e99b3ce96d" class=""><strong>Properties</strong>:</p><ul id="e037cc0c-f8c3-447a-95a3-6dfb28e87154" class="bulleted-list"><li style="list-style-type:disc"><strong>Prefix-free</strong>: No code is a prefix of another, ensuring unique decodability.</li></ul><ul id="4c092555-dd29-429b-aefd-a94f4cd9a50e" class="bulleted-list"><li style="list-style-type:disc"><strong>Optimality</strong>: Huffman coding generates the shortest possible length for prefix-free codes based on symbol frequencies.</li></ul><p id="ffff532a-f6dc-81dc-991a-f0d51d56a06d" class="">
</p></div><div id="ffff532a-f6dc-8100-b367-dcf566dc681f" style="width:100%" class="column"><figure id="ffff532a-f6dc-81c6-861c-e6a385c258e6" class="image"><a href="ebiBK.png"><img style="width:480px" src="ebiBK.png"/></a></figure><p id="7890d442-ff9c-4db4-80d1-7266015275c2" class="">
</p></div></div><p id="10fe092d-3ae5-44e2-9123-c2637b2c7952" class="">
</p></details></li></ul><ul id="584bb934-c8c3-4f59-b72f-61b0619675b1" class="toggle"><li><details open=""><summary><strong>Arithmetic Coding</strong></summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="ffff532a-f6dc-818b-a637-f7a2d8e4865d"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="5293ba57-5e0b-4308-b7b3-979c61f8082b" class="">Arithmetic coding compresses data by representing the entire message as a single fractional value between 0 and 1, using intervals based on symbol probabilities.</p></div></figure><div id="ffff532a-f6dc-811e-865d-f3d0e0dbeab7" class="column-list"><div id="0aab542f-7362-4079-b10f-887b13c285f7" style="width:75%" class="column"><p id="ffff532a-f6dc-811b-bfac-c6505ef65c42" class=""><strong>Basic Concepts</strong>:</p><ul id="ffff532a-f6dc-81ed-880b-dafcaea3e808" class="bulleted-list"><li style="list-style-type:disc"><strong>Interval Representation</strong>: Each symbol corresponds to a sub-interval within the range [0, 1] based on its probability.</li></ul><ul id="5843fc5a-1cea-417c-8bc2-ae8fd57a4d4c" class="bulleted-list"><li style="list-style-type:disc"><strong>Recursive Refinement</strong>: The message is encoded by recursively subdividing the current interval based on the probabilities of the symbols.</li></ul><ul id="ffff532a-f6dc-8129-8a58-f32f78edc2a3" class="bulleted-list"><li style="list-style-type:disc"><strong>Precision</strong>: The more symbols you encode, the smaller the interval becomes, increasing the precision of the representation.</li></ul><p id="ffff532a-f6dc-8154-91d0-c5f568d6bad6" class=""><strong>Advantages</strong>:</p><ul id="ffff532a-f6dc-8130-916b-f29d737230e3" class="bulleted-list"><li style="list-style-type:disc">Better compression efficiency than Huffman coding for some distributions.</li></ul><ul id="ffff532a-f6dc-816e-977f-d1329f0b2d1f" class="bulleted-list"><li style="list-style-type:disc">Arithmetic coding is particularly efficient when symbol probabilities are skewed.</li></ul></div><div id="ffff532a-f6dc-81a2-804c-c3f2cb19cc19" style="width:125%" class="column"><p id="ff8b5a0f-3509-41d9-a508-8ec1e6a76534" class="">
</p><figure id="c850533f-ba6b-4a40-bc26-7956ca337716"><div class="source"><a href="Note_6._Sep_2024_(2).pdf">https://prod-files-secure.s3.us-west-2.amazonaws.com/cc9fb959-9635-4863-80b9-5a928f55ffb0/0a71df96-aa06-44f4-b859-bad090223382/Note_6._Sep_2024_(2).pdf</a></div></figure></div></div></details></li></ul><ul id="ffff532a-f6dc-8198-846e-f8d99e648bea" class="toggle"><li><details open=""><summary><strong>Hamming Codes</strong></summary><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="73aa9ec6-82e6-49bd-b395-0e7559ecd7fd"><div style="font-size:1.5em"><span class="icon">💡</span></div><div style="width:100%"><p id="f2cce58f-d6d4-4376-918d-fcfb8a8fe46b" class="">Hamming codes are error-detecting and error-correcting codes, primarily used to detect and correct single-bit errors in transmitted data.</p><p id="5150dc63-017b-43f2-b6f8-3af11363a57b" class="">These codes add redundancy in a structured way that allows not just error detection but also correction.</p></div></figure><div id="ffff532a-f6dc-816f-87ce-e565f7af852a" class="column-list"><div id="bc14076d-071d-4c70-a5a8-35666fc3347f" style="width:100%" class="column"><p id="679340ce-6347-466d-9874-2f051b815a02" class=""><strong>Key Concepts</strong>:</p><ul id="c2acbc60-0643-4324-8c49-abed1703e26f" class="bulleted-list"><li style="list-style-type:disc"><strong>Redundancy</strong>: Extra bits (called parity bits) are added to the data to ensure errors can be detected and corrected.</li></ul><ul id="138f691f-22b1-4777-ac03-b1ac504b8bfa" class="bulleted-list"><li style="list-style-type:disc"><strong>Parity Bits</strong>: Inserted at positions that are powers of 2 (1, 2, 4, etc.).</li></ul><ul id="ffff532a-f6dc-81ce-b8a7-d1b9db9d4539" class="bulleted-list"><li style="list-style-type:disc"><strong>XOR Operations</strong>: Each parity bit checks certain data bits using XOR.</li></ul><ul id="d85db18f-bddf-4ec3-8ead-d3aa4e3bb5ad" class="bulleted-list"><li style="list-style-type:disc"><strong>Error Correction</strong>: Can detect and correct single-bit errors by recalculating parity bits and locating the erroneous bit.</li></ul><ul id="cf897d4d-ed0b-40d5-ac1e-f27728af934f" class="toggle"><li><details open=""><summary><strong>Steps</strong></summary><ol type="1" id="66473dfc-9fc6-4ee5-9f91-40927269c465" class="numbered-list" start="1"><li><strong>Encoding (Example Data: 1011)</strong><ul id="53187b87-fadf-4193-8625-790c10ea63d7" class="bulleted-list"><li style="list-style-type:disc">Data bits: <code>1011</code></li></ul><ul id="ffff532a-f6dc-81a4-bb8f-eef881b1eb40" class="bulleted-list"><li style="list-style-type:disc">Determine parity bits&#x27; positions (powers of 2): Positions 1, 2, and 4.</li></ul><ul id="ffff532a-f6dc-8120-9912-cf35e0576db5" class="bulleted-list"><li style="list-style-type:disc">Place data bits and parity bits in the positions:<ul id="99708ea1-2dab-47c3-b151-0fa16b6395ad" class="bulleted-list"><li style="list-style-type:circle">Position 1: Parity bit (P1)</li></ul><ul id="ffff532a-f6dc-81b1-b8c4-d5697865341d" class="bulleted-list"><li style="list-style-type:circle">Position 2: Parity bit (P2)</li></ul><ul id="62b11d57-cd38-4965-b0d5-21c180a11753" class="bulleted-list"><li style="list-style-type:circle">Position 3: Data bit (1)</li></ul><ul id="9e6b8b8b-e02c-4678-97e8-bc5acf5be222" class="bulleted-list"><li style="list-style-type:circle">Position 4: Parity bit (P3)</li></ul><ul id="ffff532a-f6dc-813e-a64e-fcb9c8142121" class="bulleted-list"><li style="list-style-type:circle">Position 5: Data bit (0)</li></ul><ul id="49f0bdd1-60d8-428a-ab1f-98cd5d55aba8" class="bulleted-list"><li style="list-style-type:circle">Position 6: Data bit (1)</li></ul><ul id="58a00a42-076a-4723-b723-a990de481404" class="bulleted-list"><li style="list-style-type:circle">Position 7: Data bit (1)</li></ul></li></ul></li></ol><ol type="1" id="7eef959f-d9e0-4035-b891-3cc2353e6c97" class="numbered-list" start="2"><li><strong>Calculate Parity Bits</strong><ul id="dc051762-a6b0-4fb0-bf0c-e2840c46dd1f" class="bulleted-list"><li style="list-style-type:disc"><strong>P1:</strong> Covers positions 1, 3, 5, 7 (i.e., P1, D1, D2, D4)<ul id="ffff532a-f6dc-8158-b18d-f50594c24953" class="bulleted-list"><li style="list-style-type:circle">To ensure even parity, P1 is set to make the count of 1s in these positions even.</li></ul></li></ul><ul id="ffff532a-f6dc-8186-8b1e-fc9cfc2df8c0" class="bulleted-list"><li style="list-style-type:disc"><strong>P2:</strong> Covers positions 2, 3, 6, 7 (i.e., P2, D1, D3, D4)<ul id="ffff532a-f6dc-81f4-855a-c4e773fb8b82" class="bulleted-list"><li style="list-style-type:circle">P2 is set to make the count of 1s in these positions even.</li></ul></li></ul><ul id="ffff532a-f6dc-81ca-94cf-c49b35d14adc" class="bulleted-list"><li style="list-style-type:disc"><strong>P3:</strong> Covers positions 4, 5, 6, 7 (i.e., P3, D2, D3, D4)<ul id="d747e73b-84a4-4b9b-9050-d279b161b300" class="bulleted-list"><li style="list-style-type:circle">P3 is set to make the count of 1s in these positions even.</li></ul></li></ul><p id="ffff532a-f6dc-81de-946f-e4de596cea62" class="">After calculating, the encoded Hamming code for <code>1011</code> might look like <code>0111011</code> (assuming parity bits calculated as shown).</p></li></ol><ol type="1" id="ffff532a-f6dc-8129-91bb-f93df4851aa0" class="numbered-list" start="3"><li><strong>Transmission and Reception</strong><ul id="ffff532a-f6dc-8186-b804-d298af14b8ce" class="bulleted-list"><li style="list-style-type:disc">Transmit <code>0111011</code> over the channel.</li></ul></li></ol><ol type="1" id="49e13283-db67-4d1a-9e92-72f415dfc7d6" class="numbered-list" start="4"><li><strong>Error Detection and Correction at Receiver</strong><ul id="862c5a61-87a8-40eb-90f2-cc972fd751ad" class="bulleted-list"><li style="list-style-type:disc"><strong>Receive:</strong> <code>0110011</code> (assume a single-bit error occurred during transmission).</li></ul><ul id="b3a533d3-6de1-40fd-aac6-3f018628cf25" class="bulleted-list"><li style="list-style-type:disc"><strong>Calculate Syndromes:</strong><ul id="ffff532a-f6dc-816a-8275-e94e89e1cd2c" class="bulleted-list"><li style="list-style-type:circle">Determine the parity bits again using the received data:<ul id="ffff532a-f6dc-81c6-b63e-cc8c5810c840" class="bulleted-list"><li style="list-style-type:square"><strong>Syndrome Calculation:</strong><ul id="8d602760-54f9-46fd-b854-1fc552a2afe8" class="bulleted-list"><li style="list-style-type:disc"><strong>S1:</strong> Checks positions 1, 3, 5, 7 (P1 parity)</li></ul><ul id="12cccbf6-8ada-4080-85b9-747769f8fbd1" class="bulleted-list"><li style="list-style-type:disc"><strong>S2:</strong> Checks positions 2, 3, 6, 7 (P2 parity)</li></ul><ul id="ffff532a-f6dc-81f2-9e84-e9a06036fc88" class="bulleted-list"><li style="list-style-type:disc"><strong>S3:</strong> Checks positions 4, 5, 6, 7 (P3 parity)</li></ul></li></ul><ul id="ffff532a-f6dc-810a-94e7-fcb741623450" class="bulleted-list"><li style="list-style-type:square">Compute parity for each set and compare with received parity bits.</li></ul></li></ul></li></ul><ul id="ffff532a-f6dc-8109-86bd-ec1804fb2be2" class="bulleted-list"><li style="list-style-type:disc"><strong>Determine Error Position:</strong><ul id="ffff532a-f6dc-815b-96ca-f650081567dd" class="bulleted-list"><li style="list-style-type:circle">Each calculated syndrome bit indicates whether there’s an error. Combine these to find the error position. For example, if S1=1, S2=1, and S3=0, it may indicate an error in position 3 (depending on the exact syndrome-to-error mapping).</li></ul></li></ul><ul id="ffff532a-f6dc-81f9-aedc-ce6cb8d75d55" class="bulleted-list"><li style="list-style-type:disc"><strong>Correct the Error:</strong><ul id="ffff532a-f6dc-81c9-ab66-d8fce46bf3a1" class="bulleted-list"><li style="list-style-type:circle">Flip the bit at the detected error position to correct it.</li></ul></li></ul></li></ol><h3 id="ffff532a-f6dc-814f-a855-c4e11fffd7af" class=""><strong>Example Error Detection:</strong></h3><p id="d614e9a9-c566-4e4d-80f4-9ee4f1f92f40" class="">Given the received data <code>0110011</code>:</p><ol type="1" id="dc6cbcaf-5d46-4fc4-982b-1026fd34ecc9" class="numbered-list" start="1"><li><strong>Calculate Syndromes:</strong><ul id="ffff532a-f6dc-81d2-8edc-f3de1ea41254" class="bulleted-list"><li style="list-style-type:disc"><strong>S1 (Parity Check for P1):</strong> Check positions 1, 3, 5, 7. The parity of these positions should match the original calculation.</li></ul><ul id="ffff532a-f6dc-81e2-b658-c2b07da09f7d" class="bulleted-list"><li style="list-style-type:disc"><strong>S2 (Parity Check for P2):</strong> Check positions 2, 3, 6, 7.</li></ul><ul id="c3d91b70-d53e-4cd4-b312-550f391e3d8f" class="bulleted-list"><li style="list-style-type:disc"><strong>S3 (Parity Check for P3):</strong> Check positions 4, 5, 6, 7.</li></ul></li></ol><ol type="1" id="3b1500ff-95ce-4039-a82f-5ddf95d49326" class="numbered-list" start="2"><li><strong>Determine Error Position:</strong><ul id="ffff532a-f6dc-814b-9144-c7f16802973c" class="bulleted-list"><li style="list-style-type:disc">Suppose the syndromes are <code>S1=1</code>, <code>S2=1</code>, <code>S3=0</code>. Combining these can indicate position 3 (binary <code>011</code>).</li></ul></li></ol><ol type="1" id="ffff532a-f6dc-8120-ac5c-fc7befb20f73" class="numbered-list" start="3"><li><strong>Correct the Error:</strong><ul id="ffff532a-f6dc-8162-8369-f57dca06f9cd" class="bulleted-list"><li style="list-style-type:disc">Flip the bit at position 3. The corrected code should be <code>0111011</code>.</li></ul></li></ol></details></li></ul></div><div id="c42abdd3-0add-4b1f-b57c-1f6577d0cbf2" style="width:100%" class="column"><figure id="ffff532a-f6dc-81c6-9fa6-e210d30949ab" class="image"><a href="ucarecdn.webp"><img style="width:528px" src="ucarecdn.webp"/></a></figure></div></div><figure id="2a8ef973-cae4-4518-94bb-0598416e58c4"><div class="source"><a href="Note_6._Sep_2024_(3).pdf">https://prod-files-secure.s3.us-west-2.amazonaws.com/cc9fb959-9635-4863-80b9-5a928f55ffb0/9addddba-2655-4d5f-9c25-1aab73b03843/Note_6._Sep_2024_(3).pdf</a></div></figure></details></li></ul></div></details><figure id="2c6147ec-282c-417e-8acd-81f1ffc527bb"><div class="source"><a href="Note_6._Sep_2024_(6).pdf">https://prod-files-secure.s3.us-west-2.amazonaws.com/cc9fb959-9635-4863-80b9-5a928f55ffb0/8b3802ea-3ebb-4baf-8d9b-4d5e57b86a2f/Note_6._Sep_2024_(6).pdf</a></div></figure></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>